{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7ae674-1a0e-459b-b9dd-11bb5f8673ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score\n",
    "import wntr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2e9f10-34f6-491d-adbd-e9b26b02674f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647cddd8-9be1-456f-9845-251620f906f6",
   "metadata": {},
   "source": [
    "The data frame *df_leaks* holds pressure measurements for different times (along rows) and different sensors (along columns '3', '10', '23', '25', '13', '14', '22', '29'). Moreover, the sensitive features and columns 'y_group1', 'y_group2' and 'y_group3' are binary labels telling us whether (1) or whether not (0) a leak is active for that time in group j for \"j = 1,2,3\". Additionally, the overall label and column 'y' is a binary label telling uns whether (1) or whether not (0) a leak is active in the WDN in general.\n",
    "\n",
    "The data frame *df_information* holds information about the leaks appearing in *df_leaks*. Each leak setting has the main characteristics 'node ID' and 'diameter'. The data *df_leaks* is generated in such a way that for each node in the WDN and each diameter 5, 10 and 15cm, there exists a period of time where a leak, defined by its location and size, is simulated. *df_information* holds information about each such setting (along rows), such as \n",
    "\n",
    "- 'group' (areal group to which the leaky node belongs to),\n",
    "- 'node ID' (location of the leak),\n",
    "- 'diameter' (size of the leak),\n",
    "- 'setting start ID' (time index in the *df_leaks* at which the setting starts),\n",
    "- 'leak start ID' (time index in the *df_leaks* at which the leak starts),\n",
    "- 'leak end ID' (time index in the *df_leaks* at which the leak ends),\n",
    "- 'setting end ID' (time index in the *df_leaks* at which the setting ends)\n",
    "\n",
    "(along columns).\n",
    "\n",
    "The data frame *df_noleaks* is of the same structure as *df_leaks* and is used as a comparison data set. There are no leaks during all time, i.e., along all rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4938cbc0-8dd0-4b5e-b828-bb2f64dfc88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_leaks = pd.read_excel('../2_DataGeneration/data_leaks.xlsx',\n",
    "                         sheet_name='leaks',\n",
    "                         index_col=0)\n",
    "df_noleaks = pd.read_excel('../2_DataGeneration/data_noleaks.xlsx',\n",
    "                           sheet_name='noleaks',\n",
    "                           index_col=0)\n",
    "df_information = pd.read_excel('../2_DataGeneration/information_leaks.xlsx',\n",
    "                               sheet_name='information',\n",
    "                               index_col=0,\n",
    "                               dtype={'node ID': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a2ac7d-08bf-4ce9-b94e-d81c58ac455e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_leaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91cbfb5-15c8-48ef-b760-d537b7f0fe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_noleaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd51f2d-1aae-4bed-95d1-e977b4c63a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8884a3-cbbd-41df-a023-c69c402d071d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Pipeline definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3f366d-416e-434c-a5b7-5b998adcf8f7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "- class to apply rolling mean along real-valued columns of a given data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b8af31-f646-4875-aa7f-d821f006cbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing_RollingMean():\n",
    "        \n",
    "    def __init__(self, time_start=0, time_wind=3):\n",
    "        if not(time_wind <= time_start):\n",
    "            print('WARNING! The transform method can be applied,',\n",
    "                  'however, the mean can not be taken over time_wind={} samples'.format(time_wind+1),\n",
    "                  'for samples with indices smaller than time_wind={}.'.format(time_wind+1))\n",
    "        self.time_start = time_start\n",
    "        self.time_wind = time_wind\n",
    "    \n",
    "    def transform(self, df):\n",
    "        \n",
    "        \"\"\"\n",
    "        Compute the rolling mean along each column of df,\n",
    "        starting at the self.time_start index row and\n",
    "        using a time window of self.time_wind + 1.\n",
    "        \n",
    "        Input:\n",
    "        df:   dataframe of shape samples x sensors\n",
    "        \n",
    "        Output:\n",
    "        X:    dataframe of shape samples x sensors\n",
    "        \"\"\"\n",
    "   \n",
    "        X = []\n",
    "        index = df.loc[time_start:,].index\n",
    "        columns = df.columns\n",
    "        for idx in index:\n",
    "            x = df.loc[idx-self.time_wind:idx,:]\n",
    "            x_mean = np.mean(np.array(x), axis=0)\n",
    "            X.append(x_mean)\n",
    "        \n",
    "        X = pd.DataFrame(data=X, \n",
    "                         index=index,\n",
    "                         columns=columns)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f205b187-0d6d-4d58-afb4-1a628a13b91b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Visualization\n",
    "\n",
    "- functionality to visualize the used water distribution network, its sensor nodes and sensitive groups\n",
    "- functionality to visualize arbitrary historical data (such as training, test and predicted data) at arbitrary sensors, based on a given time index\n",
    "- functionality to visualize arbitrary historical data (such as training, test and predicted data) at arbitrary sensors, based on a given leak settings (depending on leak location and diameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f95dc8c-30b5-4e2e-9fc4-dfc60ac656cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_network(node_ids,\n",
    "                 sensor_ids,\n",
    "                 df_information,\n",
    "                 wn,\n",
    "                 name='Hanoi',\n",
    "                 save_figs=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Input:\n",
    "    node_ids:         list of node labels\n",
    "    sensor_ids:       list of node labels of nodes which are sensors\n",
    "    df_information:   data frame which holds information about the settings\n",
    "    wn:               wntr water network\n",
    "    name:             string of the name of the wntr water network\n",
    "    save_figs:        boolean to determine whether creates grahps should be saved\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- find out what node belongs to which sensitive group\n",
    "    groups_per_node = dict()\n",
    "    for node_id in node_ids:\n",
    "\n",
    "        # find sensitive group of the node id\n",
    "        filter_node_id = df_information['node ID'] == node_id\n",
    "        index_node_id = list(df_information[filter_node_id].index)[0]\n",
    "        group = df_information.loc[index_node_id, 'group']\n",
    "        groups_per_node[node_id] = group\n",
    "        \n",
    "    # --- create color map  corresponding to each sensitive group\n",
    "    # (required for network plot)\n",
    "    nb_sensitive_features = len(set(groups_per_node.values()))\n",
    "    node_color_map = cm.get_cmap(name='Blues', lut=1000) #coolwarms\n",
    "    node_color_map = ListedColormap(node_color_map(np.linspace(0.3, 0.8, nb_sensitive_features)))\n",
    "\n",
    "    # --- create node attributes corresponding to each sensitive group\n",
    "    # (required for network plot)\n",
    "    node_attributes = dict()\n",
    "    node_attributes_sensors = dict()\n",
    "    for node_id in node_ids:\n",
    "        # this needs to be generalized to more or less sensitive groups\n",
    "        if groups_per_node[node_id]=='group1':\n",
    "            node_attributes[node_id] = 1/nb_sensitive_features\n",
    "            if node_id in sensor_ids:\n",
    "                node_attributes_sensors[node_id] = (0.6,0.6,0.6,0.5)\n",
    "        if groups_per_node[node_id]=='group2':\n",
    "            node_attributes[node_id] = 3/nb_sensitive_features\n",
    "            if node_id in sensor_ids:\n",
    "                node_attributes_sensors[node_id] = (0.6,0.6,0.6,0.5)\n",
    "        if groups_per_node[node_id]=='group3':\n",
    "            node_attributes[node_id] = 2/nb_sensitive_features\n",
    "            if node_id in sensor_ids:\n",
    "                node_attributes_sensors[node_id] = (0.6,0.6,0.6,0.5)\n",
    "\n",
    "    \n",
    "    # --- plot water network with sensor nodes and sensitve groups highlighted\n",
    "    fig, ax = plt.subplots(1, 1,\n",
    "                           # https://www.statology.org/subplot-size-matplotlib/\n",
    "                           figsize=(10,5))\n",
    "    ax.set_title('{} network and its sensor nodes and sensitive groups'.format(name),\n",
    "                 fontsize='medium', pad=0)\n",
    "    \n",
    "    # this is a bit un-nice because of the plot_network function plots the network automatically\n",
    "    wntr.graphics.plot_network(wn, \n",
    "                               node_attribute=node_attributes_sensors, \n",
    "                               node_cmap=node_color_map,\n",
    "                               node_size=800,\n",
    "                               node_labels=False,\n",
    "                               link_width=0,\n",
    "                               link_labels=False,\n",
    "                               add_colorbar=False,\n",
    "                               ax=ax)\n",
    "    wntr.graphics.plot_network(wn, \n",
    "                               node_attribute=node_attributes, \n",
    "                               node_cmap=node_color_map,\n",
    "                               node_size=350,\n",
    "                               node_labels=True,\n",
    "                               link_width=1,\n",
    "                               link_labels=False,\n",
    "                               add_colorbar=False,\n",
    "                               ax=ax)\n",
    "    \n",
    "    if save_figs:\n",
    "        fig.savefig('{}_sensornodes_{}_{}_{}.pdf'.format(name,\n",
    "                                                         sensor_ids[0],\n",
    "                                                         sensor_ids[1],\n",
    "                                                         sensor_ids[2]),\n",
    "                    format='pdf')\n",
    "        \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48db4fcd-3229-4b4a-a640-b0fff3796878",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_data_per_timeindex(df,\n",
    "                            sensor_ids,\n",
    "                            start_ids,\n",
    "                            end_ids,\n",
    "                            thresholds=None,\n",
    "                            show_legend=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Input:\n",
    "    df:              data frame\n",
    "                     which holds the to be plotted data for each sensor\n",
    "    sensor_ids:      list of sensor ids of sensors \n",
    "                     of which the data should be visualized\n",
    "    start_ids:       list of time ids of times\n",
    "                     starting from which the data should be visualized\n",
    "    end_ids:         list of time ids of times\n",
    "                     up to which the data should be visualized\n",
    "    thresholds:      dictionary with key=sensor id, value=threshold for decision gap\n",
    "    show_legend:     boolean which indicates whether or not a legend should be shown\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- plot data for each pair of start and end IDs\n",
    "    for start_id, end_id in zip(start_ids, end_ids):\n",
    "    \n",
    "        # generate time index\n",
    "        index_plot = pd.RangeIndex(start=start_id,\n",
    "                                   stop=end_id)\n",
    "\n",
    "        # -- plot data for each sensor\n",
    "        plt.figure(figsize=(22,5))\n",
    "        color_map = cm.get_cmap(name='coolwarm', lut=100)\n",
    "        sub_color_map = color_map(np.linspace(1.0, 0.0, len(sensor_ids)))\n",
    "        for index, (color, sensor_id) in enumerate(zip(sub_color_map,sensor_ids)):\n",
    "            \n",
    "            # define linestyle depending on \n",
    "            # availability of a threshold for a decision gap and\n",
    "            # the number of grafs plotted already\n",
    "            if not(thresholds == None) and (sensor_id in thresholds.keys()):\n",
    "                    linestyle = ':'\n",
    "            else:\n",
    "                if index > 2:\n",
    "                    linestyle = '--'\n",
    "                else:\n",
    "                    linestyle = '-'\n",
    "            \n",
    "            plt.plot(index_plot, df.loc[index_plot, sensor_id], \n",
    "                     label='sensor {}'.format(sensor_id), \n",
    "                     color=color, \n",
    "                     linestyle=linestyle)\n",
    "                     #alpha=1-index/(2*len(sensor_ids)))\n",
    "\n",
    "            # plot decision gap if required information is available ...\n",
    "            if not(thresholds == None):\n",
    "                # ... and the data required is the one we consider right now\n",
    "                if sensor_id in thresholds.keys():\n",
    "                    upper_decision_gap = df.loc[index_plot, sensor_id] + thresholds[sensor_id]\n",
    "                    lower_decision_gap = df.loc[index_plot, sensor_id] - thresholds[sensor_id]\n",
    "                    plt.fill_between(index_plot, upper_decision_gap, lower_decision_gap,\n",
    "                                     label='decision gap', \n",
    "                                     color=color,\n",
    "                                     alpha=0.2)\n",
    "\n",
    "        title = 'Pressure at different sensors'\n",
    "        plt.title(title)\n",
    "        plt.xlabel('time (600s)')\n",
    "        plt.ylabel('pressure')\n",
    "        if show_legend:\n",
    "            plt.legend()\n",
    "\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490b397a-f93d-4093-85d1-09666851d74a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_data_per_timeindex_and_sensor(dfs,\n",
    "                                       sensor_ids,\n",
    "                                       start_ids,\n",
    "                                       end_ids,\n",
    "                                       thresholds=None,\n",
    "                                       threshold_key=None,\n",
    "                                       show_legend=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Input:\n",
    "    dfs:             dictionary with key=plot label, value=data frame\n",
    "                     which holds the to be plotted data for each sensor\n",
    "    sensor_ids:      list of sensor ids of sensors \n",
    "                     of which the data should be visualized\n",
    "    start_ids:       list of time ids of times\n",
    "                     starting from which the data should be visualized\n",
    "    end_ids:         list of time ids of times\n",
    "                     up to which the data should be visualized\n",
    "    thresholds:      dictionary with key=sensor id, value=threshold for decision gap\n",
    "    threshold_key:   string which is some of dfs.keys() \n",
    "                     which indicates around which plot to draw a decision gap\n",
    "    show_legend:     boolean which indicates whether or not a legend should be shown\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- plot data for each pair of start and end IDs\n",
    "    for start_id, end_id in zip(start_ids, end_ids):\n",
    "    \n",
    "        # generate time index\n",
    "        index_plot = pd.RangeIndex(start=start_id,\n",
    "                                   stop=end_id)\n",
    "\n",
    "        # -- plot data for each sensor\n",
    "        plt.figure(figsize=(22,5))\n",
    "        for index, sensor_id in enumerate(sensor_ids):\n",
    "\n",
    "            plt.subplot(1, len(sensor_ids), index+1)\n",
    "\n",
    "            # -- plot data for each data frame in dfs\n",
    "            colors = ['grey', 'orange', 'blue']\n",
    "            linestyles = ['-', ':', ':']\n",
    "            alphas = [0.6, 1, 1]\n",
    "            for key, color, linestyle, alpha in zip(dfs, colors, linestyles, alphas):\n",
    "\n",
    "                plt.plot(index_plot, dfs[key].loc[index_plot, sensor_id], \n",
    "                         label=key, \n",
    "                         color=color, \n",
    "                         linestyle=linestyle, \n",
    "                         alpha=alpha)\n",
    "\n",
    "                # plot decision gap if required information is available ...\n",
    "                if not(thresholds == None) and not(threshold_key == None):\n",
    "                    # ... and the data required is the one we consider right now\n",
    "                    if (sensor_id in thresholds.keys()) and (key == threshold_key):\n",
    "                        upper_decision_gap = dfs[key].loc[index_plot, sensor_id] + thresholds[sensor_id]\n",
    "                        lower_decision_gap = dfs[key].loc[index_plot, sensor_id] - thresholds[sensor_id]\n",
    "                        plt.fill_between(index_plot, upper_decision_gap, lower_decision_gap,\n",
    "                                         label='decision gap', \n",
    "                                         color=color,\n",
    "                                         alpha=0.2)\n",
    "\n",
    "            title = 'Pressure at sensor {}'.format(sensor_id)\n",
    "            plt.title(title)\n",
    "            plt.xlabel('time (600s)')\n",
    "            plt.ylabel('pressure at sensor {}'.format(sensor_id))\n",
    "            if show_legend:\n",
    "                plt.legend()\n",
    "\n",
    "        plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd65259-e559-4ed6-8e62-ad770363f9ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_data_per_setting(df,\n",
    "                          df_information,\n",
    "                          sensor_ids,\n",
    "                          node_ids,\n",
    "                          diameters,\n",
    "                          setting_ids=None,\n",
    "                          thresholds=None,\n",
    "                          time_puffer=100,\n",
    "                          show_legend=True,\n",
    "                          zoom_leak=False,\n",
    "                          print_report=False): \n",
    "    \n",
    "    \"\"\"\n",
    "    Input:\n",
    "    df:              data frame\n",
    "                     which holds the to be plotted data for each sensor\n",
    "    df_information:  data frame which holds information about the settings\n",
    "    sensor_ids:      list of sensor ids of sensors \n",
    "                     of which the data should be visualized\n",
    "    node_ids:        list of node ids of nodes\n",
    "                     which belong to the settig that should be visualized\n",
    "    diameters:       list of diameters\n",
    "                     which belong to the settig that should be visualized\n",
    "    setting_ids:     list of setting ids of settings \n",
    "                     (combinations of node_id and diameter)\n",
    "                     which should be visualized\n",
    "                          if==None, setting_ids is generated by\n",
    "                          node_ids and diameters\n",
    "    thresholds:      dictionary with key=sensor id, value=threshold for decision gap\n",
    "    time_puffer:     float which indicates about how much the time index of the settings\n",
    "                     should be extended before and after the settings for visualization\n",
    "    show_legend:     boolean which indicates whether or not a legend should be shown\n",
    "    zoom_leak:       boolean which indicates whether of not the data should be zoomed\n",
    "                     to the leak in the specified settings\n",
    "    print_report:    boolean which indicates whether or not intermediate results should be printed\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- generate setting_ids (if necessary)\n",
    "    if setting_ids == None:\n",
    "        setting_ids = list()\n",
    "        for node_id in node_ids:\n",
    "            for diameter in diameters:\n",
    "                condition_node = df_information['node ID'] == node_id\n",
    "                condition_diameter = df_information['diameter'] == diameter\n",
    "                setting_id = list(df_information[condition_node & condition_diameter].index)[0]\n",
    "                setting_ids.append(setting_id)\n",
    "        if print_report:\n",
    "            print('Setting IDs where generated: {}'.format(setting_ids))\n",
    "    \n",
    "    # --- plot data for each setting\n",
    "    for setting_id in setting_ids:\n",
    "        \n",
    "        # access setting information\n",
    "        node_id = df_information.loc[setting_id,'node ID']\n",
    "        diameter = df_information.loc[setting_id,'diameter']\n",
    "        setting_start_id = df_information.loc[setting_id,'setting start ID']\n",
    "        leak_start_id = df_information.loc[setting_id,'leak start ID']\n",
    "        leak_end_id = df_information.loc[setting_id,'leak end ID']\n",
    "        setting_end_id = df_information.loc[setting_id,'setting end ID']\n",
    "        \n",
    "        # generate time index\n",
    "        if zoom_leak:\n",
    "            index_plot = pd.RangeIndex(start=leak_start_id-time_puffer,\n",
    "                                       stop=leak_end_id+time_puffer)\n",
    "        else:\n",
    "            index_plot = pd.RangeIndex(start=setting_start_id-time_puffer,\n",
    "                                       stop=setting_end_id+time_puffer)\n",
    "     \n",
    "        # -- plot data for each sensor\n",
    "        plt.figure(figsize=(22,5))\n",
    "        color_map = cm.get_cmap(name='coolwarm', lut=100)\n",
    "        sub_color_map = color_map(np.linspace(1.0, 0.0, len(sensor_ids)))\n",
    "        for index, (color, sensor_id) in enumerate(zip(sub_color_map,sensor_ids)):\n",
    "            \n",
    "            # define linestyle depending on \n",
    "            # availability of a threshold for a decision gap and\n",
    "            # the number of grafs plotted already\n",
    "            if not(thresholds == None) and (sensor_id in thresholds.keys()):\n",
    "                    linestyle = ':'\n",
    "            else:\n",
    "                if index > 2:\n",
    "                    linestyle = '--'\n",
    "                else:\n",
    "                    linestyle = '-'\n",
    "                \n",
    "            plt.plot(index_plot, df.loc[index_plot, sensor_id], \n",
    "                     label='sensor {}'.format(sensor_id), \n",
    "                     color=color, \n",
    "                     linestyle=linestyle)\n",
    "                     #alpha=1-index/(2*len(sensor_ids)))\n",
    "\n",
    "            # plot decision gap if required information is available ...\n",
    "            if not(thresholds == None): \n",
    "                # ... and the data required is the one we consider right now\n",
    "                if sensor_id in thresholds.keys():\n",
    "                    upper_decision_gap = df.loc[index_plot, sensor_id] + thresholds[sensor_id]\n",
    "                    lower_decision_gap = df.loc[index_plot, sensor_id] - thresholds[sensor_id]\n",
    "                    plt.fill_between(index_plot, upper_decision_gap, lower_decision_gap,\n",
    "                                     label='decision gap', \n",
    "                                     color=color,\n",
    "                                     alpha=0.2)\n",
    "\n",
    "            # plot start and end of leak\n",
    "            plt.scatter([leak_start_id, leak_end_id], \n",
    "                        [df.loc[leak_start_id, sensor_id],\n",
    "                         df.loc[leak_end_id, sensor_id]],\n",
    "                         label='leak start and end',\n",
    "                         color=color,\n",
    "                         marker='o')\n",
    "\n",
    "        title = 'Pressure at different sensors'\n",
    "        title += '\\nLeak location: Node {}'.format(node_id)\n",
    "        title += '\\nLeak diameter: {} cm'.format(diameter)\n",
    "        plt.title(title)\n",
    "        plt.xlabel('time (600s)')\n",
    "        plt.ylabel('pressure')\n",
    "        if show_legend:\n",
    "            plt.legend()\n",
    "\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfef5a26-0d94-490b-bce4-f9b1cbc29a4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_data_per_setting_and_sensor(dfs,\n",
    "                                     df_information,\n",
    "                                     sensor_ids,\n",
    "                                     node_ids,\n",
    "                                     diameters,\n",
    "                                     setting_ids=None,\n",
    "                                     thresholds=None,\n",
    "                                     threshold_key=None,\n",
    "                                     leak_key=None,\n",
    "                                     time_puffer=100,\n",
    "                                     show_legend=True,\n",
    "                                     zoom_leak=False,\n",
    "                                     print_report=False): \n",
    "    \n",
    "    \"\"\"\n",
    "    Input:\n",
    "    dfs:             dictionary with key=plot label, value=data frame\n",
    "                     which holds the to be plotted data for each sensor\n",
    "    df_information:  data frame which holds information about the settings\n",
    "    sensor_ids:      list of sensor ids of sensors \n",
    "                     of which the data should be visualized\n",
    "    node_ids:        list of node ids of nodes\n",
    "                     which belong to the settig that should be visualized\n",
    "    diameters:       list of diameters\n",
    "                     which belong to the settig that should be visualized\n",
    "    setting_ids:     list of setting ids of settings \n",
    "                     (combinations of node_id and diameter)\n",
    "                     which should be visualized\n",
    "                          if==None, setting_ids is generated by\n",
    "                          node_ids and diameters\n",
    "    thresholds:      dictionary with key=sensor id, value=threshold for decision gap\n",
    "    threshold_key:   string which is some of dfs.keys() \n",
    "                     which indicates around which plot to draw a decision gap\n",
    "    leak_key:        string which is some of dfs.keys() \n",
    "                     which indicates on which plot to draw the leak start and end\n",
    "    time_puffer:     float which indicates about how much the time index of the settings\n",
    "                     should be extended before and after the settings for visualization\n",
    "    show_legend:     boolean which indicates whether or not a legend should be shown\n",
    "    zoom_leak:       boolean which indicates whether of not the data should be zoomed\n",
    "                     to the leak in the specified settings\n",
    "    print_report:    boolean which indicates whether or not intermediate results should be printed\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- generate setting_ids (if necessary)\n",
    "    if setting_ids == None:\n",
    "        setting_ids = list()\n",
    "        for node_id in node_ids:\n",
    "            for diameter in diameters:\n",
    "                condition_node = df_information['node ID'] == node_id\n",
    "                condition_diameter = df_information['diameter'] == diameter\n",
    "                setting_id = list(df_information[condition_node & condition_diameter].index)[0]\n",
    "                setting_ids.append(setting_id)\n",
    "        if print_report:\n",
    "            print('Setting IDs where generated: {}'.format(setting_ids))\n",
    "    \n",
    "    # --- plot data for each setting\n",
    "    for setting_id in setting_ids:\n",
    "        \n",
    "        # access setting information\n",
    "        node_id = df_information.loc[setting_id,'node ID']\n",
    "        diameter = df_information.loc[setting_id,'diameter']\n",
    "        setting_start_id = df_information.loc[setting_id,'setting start ID']\n",
    "        leak_start_id = df_information.loc[setting_id,'leak start ID']\n",
    "        leak_end_id = df_information.loc[setting_id,'leak end ID']\n",
    "        setting_end_id = df_information.loc[setting_id,'setting end ID']\n",
    "        \n",
    "        # generate time index\n",
    "        if zoom_leak:\n",
    "            index_plot = pd.RangeIndex(start=leak_start_id-time_puffer,\n",
    "                                       stop=leak_end_id+time_puffer)\n",
    "        else:\n",
    "            index_plot = pd.RangeIndex(start=setting_start_id-time_puffer,\n",
    "                                       stop=setting_end_id+time_puffer)\n",
    "     \n",
    "        # -- plot data for each sensor\n",
    "        plt.figure(figsize=(22,5))\n",
    "        for index, sensor_id in enumerate(sensor_ids):\n",
    "            \n",
    "            plt.subplot(1, len(sensor_ids), index+1)\n",
    "            \n",
    "            # -- plot data for each data frame in dfs\n",
    "            colors = ['grey', 'orange', 'blue']\n",
    "            linestyles = ['-', ':', ':']\n",
    "            alphas = [0.6, 1, 1]\n",
    "            for key, color, linestyle, alpha in zip(dfs, colors, linestyles, alphas):\n",
    "                \n",
    "                plt.plot(index_plot, dfs[key].loc[index_plot, sensor_id], \n",
    "                         label=key,\n",
    "                         color=color, \n",
    "                         linestyle=linestyle, \n",
    "                         alpha=alpha)\n",
    "            \n",
    "                # plot decision gap if required information is available ...\n",
    "                if not(thresholds == None) and not(threshold_key == None):\n",
    "                    # ... and the data required is the one we consider right now\n",
    "                    if (sensor_id in thresholds.keys()) and (key == threshold_key):\n",
    "                        upper_decision_gap = dfs[key].loc[index_plot, sensor_id] + thresholds[sensor_id]\n",
    "                        lower_decision_gap = dfs[key].loc[index_plot, sensor_id] - thresholds[sensor_id]\n",
    "                        plt.fill_between(index_plot, upper_decision_gap, lower_decision_gap,\n",
    "                                         label='decision gap', \n",
    "                                         color=color,\n",
    "                                         alpha=0.2)\n",
    "                        \n",
    "                # plot start and end of leak if required information is available ...\n",
    "                if not(leak_key == None):\n",
    "                    # ... and the data required is the one we consider right now\n",
    "                    if key == leak_key:\n",
    "                        plt.scatter([leak_start_id, leak_end_id], \n",
    "                                    [dfs[key].loc[leak_start_id, sensor_id],\n",
    "                                     dfs[key].loc[leak_end_id, sensor_id]],\n",
    "                                     label='leak start and end',\n",
    "                                     color=color,\n",
    "                                     marker='o')\n",
    "\n",
    "            title = 'Pressure at sensor {}'.format(sensor_id)\n",
    "            title += '\\nLeak location: Node {}'.format(node_id)\n",
    "            title += '\\nLeak diameter: {} cm'.format(diameter)\n",
    "            plt.title(title)\n",
    "            plt.xlabel('time (600s)')\n",
    "            plt.ylabel('pressure at sensor {}'.format(sensor_id))\n",
    "            if show_legend:\n",
    "                plt.legend()\n",
    "            \n",
    "        plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfbe38a-3713-44f3-aeba-41f2bae58690",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Regression - Virtual Sensors\n",
    "\n",
    "- class to train and test a multi regression model, i.e., a regression model per column of a given data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01097c36-101a-418b-9c03-09aea9c5a2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiRegression():\n",
    "    \n",
    "    def __init__(self, regressor):\n",
    "        \n",
    "        \"\"\"\n",
    "        Input:\n",
    "        regressor:  regression class\n",
    "        \"\"\"\n",
    "        \n",
    "        self.regressor = regressor\n",
    "        self.regressors = dict()\n",
    "        \n",
    "    def fit(self, X_train, Y_train, print_coeff=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Train regression model (virtual sensor) per sensor node\n",
    "        based on the (pressure) inputs of the other sensor nodes.\n",
    "        \n",
    "        Input:\n",
    "        X_train:      dataframe of training (pressure) inputs \n",
    "                      of shape samples x sensors\n",
    "        Y_train:      dataframe of training (pressure) labels \n",
    "                      of shape samples x sensors\n",
    "        print_coeff:  boolean which indicates whether trained coefficients\n",
    "                      should be printed or not\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- train one regression model (virtual sensor) per sensor node\n",
    "        sensor_ids = list(X_train.columns)\n",
    "        for node in sensor_ids:\n",
    "            \n",
    "            # -- extract sensor node specific training data\n",
    "            columns = sensor_ids.copy()\n",
    "            columns.remove(node)\n",
    "            X_train_node = X_train.loc[:,columns] \n",
    "            y_train_node = Y_train.loc[:,[node]]\n",
    "            \n",
    "            # -- train regression model (virtual sensor)\n",
    "            model = self.regressor()\n",
    "            model.fit(X_train_node, y_train_node)\n",
    "            if print_coeff:\n",
    "                print('Model for sensor node {}:\\n'\\\n",
    "                      'Coef.: {}, Intercept: {}.'.format(node,\n",
    "                                                         model.coef_,\n",
    "                                                         model.intercept_))\n",
    "                \n",
    "            # -- store trained regression model (virtual sensor) \n",
    "            self.regressors[node] = dict()\n",
    "            self.regressors[node]['regressor'] = model\n",
    "            self.regressors[node]['coef_'] = model.coef_\n",
    "            self.regressors[node]['intercept_'] = model.intercept_\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        \n",
    "        \"\"\"\n",
    "        Predict (pressure) output per sensor node\n",
    "        based on the (pressure) inputs of the other sensor nodes.\n",
    "        \n",
    "        Input:\n",
    "        X_test:      dataframe of test (pressure) inputs \n",
    "                     of shape samples x sensors\n",
    "        \n",
    "        Output:\n",
    "        Y_pred:      dataframe of test (pressure) labels \n",
    "                     of shape samples x sensors\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- predict output (of virtual sensor) per sensor node\n",
    "        sensor_ids = list(X_test.columns)\n",
    "        Y_pred = pd.DataFrame(index=X_test.index,\n",
    "                              columns=X_test.columns)\n",
    "        for node in sensor_ids:\n",
    "            \n",
    "            # -- extract sensor node specific test data\n",
    "            columns = sensor_ids.copy()\n",
    "            columns.remove(node)\n",
    "            X_test_node = X_test.loc[:,columns] \n",
    "            \n",
    "            # -- access trained regression model (virtual sensor)\n",
    "            model = self.regressors[node]['regressor']\n",
    "            \n",
    "            # -- predict output (of virtual sensor)\n",
    "            y_pred = model.predict(X_test_node)\n",
    "            y_pred = pd.DataFrame(data=y_pred,\n",
    "                                  index=X_test_node.index,\n",
    "                                  columns=[node])\n",
    "            \n",
    "            # -- store regression prediction (of virtual sensors)\n",
    "            Y_pred.loc[:,node] = y_pred\n",
    "            \n",
    "        return Y_pred\n",
    "    \n",
    "    def score(self, X_test, Y_test, print_all_scores=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Compute R2 score and RMSE per sensor node.\n",
    "        \n",
    "        Input:\n",
    "        X_test:             dataframe of test (pressure) inputs \n",
    "                            of shape samples x sensors\n",
    "        Y_test:             dataframe of test (pressure) labels \n",
    "                            of shape samples x sensors\n",
    "        print_all_scores:   boolean which indicates whether scores\n",
    "                            should be printed or not\n",
    "                            \n",
    "        Output:\n",
    "        mean_r2:            mean R2 score over all sensor nodes\n",
    "                            and over all samples\n",
    "        mean_rmse:          mean RMSE over all sensor nodes\n",
    "                            over all samples\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- predict output (of virtual sensor) per sensor node\n",
    "        Y_pred = self.predict(X_test)\n",
    "        \n",
    "        # --- calculate scores per sensor node\n",
    "        sensor_ids = list(X_test.columns)\n",
    "        scores = pd.DataFrame(index=Y_test.columns,\n",
    "                              columns=['rmse','r2'])\n",
    "        for node in sensor_ids:\n",
    "            \n",
    "            # -- extract sensor node specific test and predicted data\n",
    "            y_test = Y_test.loc[:,node]\n",
    "            y_pred = Y_pred.loc[:,node]\n",
    "            \n",
    "            # -- store scores\n",
    "            scores.loc[node,'rmse'] = mean_squared_error(y_test, y_pred, \n",
    "                                                         squared=False)\n",
    "            scores.loc[node,'r2'] = r2_score(y_test, y_pred)\n",
    "        \n",
    "        if print_all_scores:\n",
    "            print('All scores:\\n', scores)\n",
    "        mean_r2 = scores.loc[:,'r2'].mean()\n",
    "        mean_rmse = scores.loc[:,'rmse'].mean()\n",
    "        \n",
    "        return mean_r2, mean_rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03127c0d-3771-49c3-b2a2-7a582b647009",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Classification - Leak Dectector(s)\n",
    "\n",
    "- class to apply a threshold based classfication model that can be integrated into the multi classification resp. ensemble classification class\n",
    "- class to train and test a multi classfication model, i.e., a classification model per column of a given data frame, resp. an ensemble classification model\n",
    "- different subclasses of the multi classification resp. ensemble classification class according to different training algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc7f8ef-d9fb-4b6f-b073-3cf93fed03d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some minor functions\n",
    "def log_on_R(x):\n",
    "    if x <= 0:\n",
    "        return -1 * np.inf\n",
    "    else:\n",
    "        return np.log(x)\n",
    "\n",
    "def sigmoid(x,b):\n",
    "    return 1/(1+np.exp(-b*x))\n",
    "\n",
    "def dx_sigmoid(x,b):\n",
    "    return b * sigmoid(x,b) * (1-sigmoid(x,b))\n",
    "\n",
    "def rates(y,f):\n",
    "    #Input:\n",
    "    #y, f:   dataframes of shape nodes x 1\n",
    "    #        and with the same column name\n",
    "    if float(y.sum()) != 0:\n",
    "        rate = (y*f).sum() / y.sum()\n",
    "        return float(rate)\n",
    "    else:\n",
    "        return '-'\n",
    "\n",
    "def TPR(y_test,y_pred):\n",
    "    #Input:\n",
    "    #y_test, y_pred:   dataframes of shape nodes x 1\n",
    "    #                  and with the same column name\n",
    "    return rates(y_test,y_pred)\n",
    "\n",
    "def FPR(y_test,y_pred):\n",
    "    return rates(1-y_test,y_pred)\n",
    "\n",
    "def FNR(y_test,y_pred):\n",
    "    return rates(y_test,1-y_pred)\n",
    "\n",
    "def TNR(y_test,y_pred):\n",
    "    return rates(1-y_test,1-y_pred)\n",
    "\n",
    "def PPV(y_test,y_pred):\n",
    "    return rates(y_pred,y_test)\n",
    "\n",
    "def ACC(y_test,y_pred):\n",
    "    acc = (y_test*y_pred).sum() + ((1-y_test)*(1-y_pred)).sum()\n",
    "    acc /= len(y_test)\n",
    "    return float(acc)\n",
    "\n",
    "def dx_ACC(y_test,dx_y_pred):\n",
    "    dx_acc = (((2*y_test)-1)*dx_y_pred).sum()\n",
    "    dx_acc /= len(y_test)\n",
    "    return float(dx_acc)\n",
    "\n",
    "def Cov(x_sen,y_pred):\n",
    "    #Input:\n",
    "    #x_sen, y_pred:   series of the shape nodes x \n",
    "    return x_sen.cov(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f61a104-f36c-450b-be93-e0d0c48d8ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold classification classes \n",
    "# (based on residual inputs)\n",
    "\n",
    "class ThresholdClassification():\n",
    "    \n",
    "    def __init__(self, threshold):\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    # no fit method since this model is used in an esemble \n",
    "    # and training techniques might optimize over all esemble learner\n",
    "        \n",
    "    def predict(self, x_test):\n",
    "        \n",
    "        \"\"\"\n",
    "        Predict (leak y/n) output for one sensor node.\n",
    "        \n",
    "        Input:\n",
    "        x_test:   series of test (pressure) residual inputs\n",
    "        \n",
    "        Output:\n",
    "        y_pred:   series of test (leak y/n) outputs\n",
    "        \"\"\"\n",
    "        \n",
    "        find_leak = lambda x : 1 if (x > self.threshold) else 0\n",
    "        y_pred = x_test.apply(find_leak)\n",
    "        return y_pred\n",
    "    \n",
    "class ThresholdClassificationApproximation():\n",
    "    \n",
    "    def __init__(self,threshold):\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    # no fit method since this model is used in an esemble \n",
    "    # and training techniques might optimize over all esemble learner\n",
    "        \n",
    "    def predict(self, x_test, b_sigmoid):\n",
    "        \n",
    "        \"\"\"\n",
    "        Predict approximated (leak y/n) output for one sensor node.\n",
    "        \n",
    "        Input:\n",
    "        x_test:   series of test (pressure) residuals inputs\n",
    "        \n",
    "        Output:\n",
    "        y_pred:   series of test (leak y/n) outputs\n",
    "        \"\"\"\n",
    "        \n",
    "        y_pred = x_test.apply(lambda x: x - self.threshold)\n",
    "        y_pred = y_pred.apply(sigmoid, b=b_sigmoid)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5ae069-4fa5-4cca-bb36-169ffc1ecae5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# threshold classification ensemble super classes\n",
    "# (based on residual inputs)\n",
    "\n",
    "class EnsembleThresholdClassification():\n",
    "    \n",
    "    def __init__(self, classifier, classifier_approx):\n",
    "        \n",
    "        \"\"\"\n",
    "        Input:\n",
    "        classifier:          residual based threshold classifier class\n",
    "        classifier_approx:   residual based approximative threshold \n",
    "                             classifier class\n",
    "        \"\"\"\n",
    "        \n",
    "        self.classifier = classifier\n",
    "        self.classifier_approx = classifier_approx\n",
    "        self.thresholds = dict()\n",
    "        self.classifiers = dict()\n",
    "        self.classifiers_approx = dict()\n",
    "        \n",
    "    def fit(self):\n",
    "        print('No fitting method implemented yet')\n",
    "        \n",
    "    def fit_model(self, thresholds, print_coeff=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Train residual based threshold classification model (leak detector)\n",
    "        per sensor node\n",
    "        based on the trained thresholds in .fit() method.\n",
    "        \n",
    "        Input:\n",
    "        thresholds:    dictionary with keys:node, values:(trained) threshold\n",
    "        print_coeff:   boolean which indicates whether trained coefficients\n",
    "                       should be printed or not\n",
    "        \"\"\"\n",
    "        \n",
    "        # -- store trained classification model (leak detector) per sensor node\n",
    "        for node, threshold in thresholds.items():\n",
    "            if print_coeff:\n",
    "                print('Sensor: {}, Threshold: {}.'.format(node,threshold))\n",
    "            model = self.classifier(threshold=threshold)\n",
    "            model_approx = self.classifier_approx(threshold=threshold)\n",
    "            self.classifiers[node] = dict()\n",
    "            self.classifiers[node]['classifier'] = model\n",
    "            self.classifiers[node]['threshold'] = threshold\n",
    "            self.classifiers_approx[node] = dict()\n",
    "            self.classifiers_approx[node]['classifier'] = model_approx\n",
    "            self.classifiers_approx[node]['threshold'] = threshold\n",
    "        \n",
    "    def predict(self, \n",
    "                X_test,\n",
    "                keys_list=None,\n",
    "                thresholds_array=None):\n",
    "        \n",
    "        \"\"\"\n",
    "        Predict (leak y/n) output per sensor node \n",
    "        and as an ensemble (leak y/n) output over all sensors\n",
    "        based on the (pressure) residual inputs per sensor node.\n",
    "        \n",
    "        Input:\n",
    "        X_test:             dataframe of test (pressure) residual inputs \n",
    "                            of shape samples x sensors\n",
    "        keys_list:          None or list of keys corresponding to each sensor node\n",
    "                            of shape sensors x 1\n",
    "                            and in the order of the corresponding thresholds \n",
    "                            in the thresholds_array\n",
    "                            (if None, self.thresholds.keys() is used)\n",
    "        thresholds_array:   None or numpy array of thresholds used for each sensor node\n",
    "                            of shape nodes x 1\n",
    "        \n",
    "        Output:\n",
    "        Y_pred:             dataframe of test (leak y/n) labels \n",
    "                            of shape samples x sensors\n",
    "        y_pred:             dataframe of test (leak y/n) labels\n",
    "                            of shape samples x 1\n",
    "                            \n",
    "        For training purposes, this method is used for specified \n",
    "        keys_list and thresholds_array,\n",
    "        for prediction purposes, thess remain None and\n",
    "        self.thresholds \n",
    "        is automatically used instead.\n",
    "        \"\"\"\n",
    "        \n",
    "        if (keys_list!=None) and (list(thresholds_array)!=None):\n",
    "            thresholds = dict(zip(keys_list, thresholds_array))\n",
    "        else:\n",
    "            thresholds = self.thresholds\n",
    "        \n",
    "        # --- predict output (of leak detector) per sensor node\n",
    "        sensor_ids = list(X_test.columns)\n",
    "        Y_pred = pd.DataFrame(index=X_test.index,\n",
    "                              columns=X_test.columns)\n",
    "        for node in sensor_ids:\n",
    "            \n",
    "            # -- extract sensor node specific test data\n",
    "            x_test_node = X_test.loc[:,node] \n",
    "            \n",
    "            # -- access (trained) classification model (leak detector)\n",
    "            threshold = thresholds[node]\n",
    "            model = self.classifier(threshold=threshold)\n",
    "            \n",
    "            # -- predict output (of leak detector)\n",
    "            y_pred = model.predict(x_test_node)\n",
    "            \n",
    "            # -- store classification prediction (of leak detector)\n",
    "            Y_pred.loc[:,node] = y_pred\n",
    "            \n",
    "        # --- predict output (of leak detector) over all sensor nodes\n",
    "        # predict a leak if at last one classification model \n",
    "        # per node predicts a leak\n",
    "        Y_pred['sum'] =  Y_pred.agg(func='sum', axis=1)\n",
    "        find_leak = lambda su : 1 if (su >= 1) else 0\n",
    "        y_pred = Y_pred['sum'].apply(find_leak)\n",
    "        y_pred = pd.DataFrame(data=y_pred,\n",
    "                              index=y_pred.index).rename({'sum':'y'},\n",
    "                                                         axis='columns')\n",
    "                \n",
    "        return Y_pred, y_pred\n",
    "    \n",
    "    def predict_approx(self, \n",
    "                       X_test, \n",
    "                       keys_list=None,\n",
    "                       thresholds_array=None,\n",
    "                       b_sigmoid=100, \n",
    "                       sum_threshold=0.8):\n",
    "        \n",
    "        \"\"\"\n",
    "        Predict approximated (leak y/n) output per sensor node \n",
    "        and as an ensemble (leak y/n) output over all sensors\n",
    "        based on the (pressure) residual inputs per sensor node.\n",
    "        \n",
    "        Input:\n",
    "        X_test:             dataframe of test (pressure) residual inputs \n",
    "                            of shape samples x sensors\n",
    "        keys_list:          None or list of keys corresponding to each sensor node\n",
    "                            of shape sensors x 1\n",
    "                            and in the order of the corresponding thresholds \n",
    "                            in the thresholds_array\n",
    "                            (if None, self.thresholds.keys() is used)\n",
    "        thresholds_array:   None or numpy array of thresholds used for each sensor node\n",
    "                            of shape nodes x 1\n",
    "                            (if None, self.thresholds.keys() is used)\n",
    "        b_sigmoid:          positive float that indicates \n",
    "                            the b in the sigmoid function\n",
    "        sum_threshold:      positive float that indicates\n",
    "                            when the sum of node inputs indicates a leak\n",
    "        \n",
    "        Output:\n",
    "        Y_pred:             dataframe of test (leak y/n) labels \n",
    "                            of shape samples x sensors\n",
    "        y_pred:             dataframe of test (leak y/n) labels\n",
    "                            of shape samples x 1\n",
    "                            \n",
    "        For training purposes, this method is used for specified \n",
    "        keys_list and thresholds_array,\n",
    "        for prediction purposes, thess remain None and\n",
    "        self.thresholds \n",
    "        is automatically used instead.\n",
    "        \"\"\"\n",
    "        \n",
    "        if (keys_list!=None) and (list(thresholds_array)!=None):\n",
    "            thresholds = dict(zip(keys_list, thresholds_array))\n",
    "        else:\n",
    "            thresholds = self.thresholds\n",
    "        \n",
    "        # --- approximately predict output (of leak detector) per sensor node\n",
    "        sensor_ids = list(X_test.columns)\n",
    "        Y_pred = pd.DataFrame(index=X_test.index,\n",
    "                              columns=X_test.columns)\n",
    "        for node in sensor_ids:\n",
    "            \n",
    "            # -- extract sensor node specific test data\n",
    "            x_test_node = X_test.loc[:,node] \n",
    "            \n",
    "            # -- access (trained) approximative classification model (leak detector)\n",
    "            threshold = thresholds[node]\n",
    "            model = self.classifier_approx(threshold=threshold)\n",
    "            \n",
    "            # -- predict output (of leak detector)\n",
    "            y_pred = model.predict(x_test_node, b_sigmoid)\n",
    "            \n",
    "            # -- store classification prediction (of leak detector)\n",
    "            Y_pred.loc[:,node] = y_pred\n",
    "            \n",
    "        # --- approximately predict output (of leak detector) over all sensor nodes\n",
    "        # predict a leak if all classification models over all nodes\n",
    "        # in sum predicts a value larger than sum_thresholds\n",
    "        Y_pred['sum'] = Y_pred.agg(func='sum', axis=1)\n",
    "        y_pred = Y_pred['sum'].apply(lambda su: su - sum_threshold)\n",
    "        y_pred = y_pred.apply(sigmoid, b=b_sigmoid)\n",
    "        y_pred = pd.DataFrame(data=y_pred,\n",
    "                              index=y_pred.index).rename({'sum':'y'},\n",
    "                                                         axis='columns')\n",
    "                \n",
    "        return Y_pred, y_pred\n",
    "    \n",
    "    def dx_predict_approx(self, \n",
    "                          X_test, \n",
    "                          keys_list=None,\n",
    "                          thresholds_array=None,\n",
    "                          b_sigmoid=100, \n",
    "                          sum_threshold=0.8):\n",
    "        \n",
    "        \"\"\"\n",
    "        Compute gradient with respect to thresholds per sensor node\n",
    "        of approximated (leak y/n) output prediction per sensor node \n",
    "        based on the (pressure) residual inputs per sendor node.\n",
    "        \n",
    "        Input:\n",
    "        X_test:             dataframe of test (pressure) residual inputs \n",
    "                            of shape samples x sensors\n",
    "        keys_list:          None or list of keys corresponding to each sensor node\n",
    "                            of shape sensors x 1\n",
    "                            and in the order of the corresponding thresholds \n",
    "                            in the thresholds_array\n",
    "                            (if None, self.thresholds.keys() is used)\n",
    "        thresholds_array:   None or numpy array of thresholds used for each sensor node\n",
    "                            of shape nodes x 1\n",
    "                            (if None, self.thresholds.keys() is used)\n",
    "        b_sigmoid:          positive float that indicates \n",
    "                            the b in the sigmoid function\n",
    "        sum_threshold:      positive float that indicates\n",
    "                            when the sum of sensor node outputs indicates a leak\n",
    "        \n",
    "        Output:\n",
    "        dx_Y_pred:         dataframe of partial derivatives of the\n",
    "                           test (leak y/n) labels \n",
    "                           with respect to the thresholds per sensor node\n",
    "                           of shape samples x sensors\n",
    "                           \n",
    "        For training purposes, this method is used for specified \n",
    "        keys_list and thresholds_array,\n",
    "        for prediction purposes, thess remain None and\n",
    "        self.thresholds \n",
    "        is automatically used instead.\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- approximately predict output (of leak detector) \n",
    "        # --- per sensor node and over all sensors\n",
    "        Y_pred, y_pred = self.predict_approx(X_test,\n",
    "                                             keys_list=keys_list,\n",
    "                                             thresholds_array=thresholds_array,\n",
    "                                             b_sigmoid=b_sigmoid, \n",
    "                                             sum_threshold=sum_threshold)\n",
    "        \n",
    "        # --- compute partial derivatives of approximately predicted output \n",
    "        # --- wrt. each sensor node's threshold\n",
    "        # -- precompute factors which are the same for each partial derivative\n",
    "        # (this is the \"outer\" derivative)\n",
    "        factor = - b_sigmoid**2 * y_pred * (1 - y_pred)\n",
    "        \n",
    "        # -- precompute factors which are sensor specific\n",
    "        # (this is the \"inner\" derivative)\n",
    "        factor_nodes = Y_pred * (1 - Y_pred)\n",
    "        \n",
    "        # -- combine outer and inner derivative per sensor node\n",
    "        sensor_ids = list(X_test.columns)\n",
    "        dx_Y_pred = pd.DataFrame(index=X_test.index,\n",
    "                                 columns=X_test.columns)\n",
    "        for node in sensor_ids:\n",
    "            \n",
    "            # rename first factor to make pandas series multiplication possible\n",
    "            factor_node_1 = factor.rename({'y':node},\n",
    "                                          axis='columns')\n",
    "            # access inner derivative of approximately predicted output \n",
    "            # wrt. to sensors's threshold\n",
    "            factor_node_2 = factor_nodes.loc[:,[node]]\n",
    "            \n",
    "            # store partial derivatives of approximately predicted output \n",
    "            dx_Y_pred.loc[:,node] = factor_node_1 * factor_node_2\n",
    "            \n",
    "        return dx_Y_pred\n",
    "            \n",
    "    \n",
    "    def score(self, \n",
    "              X_test, \n",
    "              y_test,\n",
    "              keys_list=None,\n",
    "              thresholds_array=None,\n",
    "              print_all_scores=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Compute TPR, FNR, FPR, TNR, PPV and relative amout of positive predictions\n",
    "        with respect to ensemble (leak y/n) output over all sensors\n",
    "        based on the (pressure) residual inputs per sensor node.\n",
    "        \n",
    "        Input:\n",
    "        X_test:             dataframe of test (pressure) residual inputs \n",
    "                            of shape samples x sensors\n",
    "        y_test:             dataframe of test (leak y/n) labels \n",
    "                            of shape samples x 1\n",
    "        keys_list:          None or list of keys corresponding to each sensor node\n",
    "                            of shape sensors x 1\n",
    "                            and in the order of the corresponding thresholds \n",
    "                            in the thresholds_array\n",
    "                            (if None, self.thresholds.keys() is used)\n",
    "        thresholds_array:   None or numpy array of thresholds used for each sensor node\n",
    "                            of shape nodes x 1\n",
    "        print_all_scores:   boolean which indicates whether scores\n",
    "                            should be printed or not\n",
    "                            \n",
    "        Output:\n",
    "        tpr:                float of TPR over all samples\n",
    "        fnr:                float of FNR over all samples\n",
    "        fpr:                float of FPR over all samples\n",
    "        tnr:                float of TNR over all samples\n",
    "        ppv:                float of PPV score over all samples\n",
    "        acc:                float of accuracy over all samples\n",
    "        pos_pred:           float of amount of positive predictions \n",
    "                            over all samples\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- predict output (of leak detector) over all sensor nodes\n",
    "        _, y_pred = self.predict(X_test,\n",
    "                                 keys_list=keys_list,\n",
    "                                 thresholds_array=thresholds_array)\n",
    "        \n",
    "        # --- calculate scores\n",
    "        tpr = TPR(y_test,y_pred) # tp/(tp+fn)\n",
    "        fnr = FNR(y_test,y_pred) # fn/(tp+fn)\n",
    "        fpr = FPR(y_test,y_pred) # fp/(fp+tn)\n",
    "        tnr = TNR(y_test,y_pred) # tn/(fp+tn)\n",
    "        ppv = PPV(y_test,y_pred) # tp/(tp+fp)\n",
    "        acc = ACC(y_test,y_pred) # (tp+tn)/(tp+fn+fp+tn)\n",
    "        pos_pred = float(y_pred.sum())/len(y_pred) # (tp+fp)/(tp+fn+fp+tn)\n",
    "        \n",
    "        if print_all_scores:\n",
    "            print('TPR:', tpr)\n",
    "            print('FNR = 1 - TPR:', fnr)\n",
    "            print('FPR:', fpr)\n",
    "            print('TNR = 1 - FPR:', tnr)\n",
    "            print('PPV:', ppv)\n",
    "            print('Acc.:', acc)\n",
    "            print('Rel. pos. predictions:', pos_pred)\n",
    "            print('TPR - FPR:', tpr-fpr)\n",
    "        \n",
    "        return tpr, fnr, fpr, tnr, ppv, acc, pos_pred\n",
    "    \n",
    "    def score_fairness(self, \n",
    "                       X_test, \n",
    "                       X_sen_test, \n",
    "                       y_test, \n",
    "                       keys_list=None,\n",
    "                       thresholds_array=None,\n",
    "                       print_all_scores=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Compute TPR, equal opportunity and disparate impact\n",
    "        with respect to ensemble (leak y/n) output over all sensors\n",
    "        based on the (pressure) residual inputs per sensor node.\n",
    "        \n",
    "        Input:\n",
    "        X_test:             dataframe of test (pressure) residual inputs \n",
    "                            of shape samples x sensors\n",
    "        X_sen_test:         dataframe of test sensitive (feature) inputs \n",
    "                            of shape samples x sensitive features\n",
    "        y_test:             dataframe of test (leak y/n) labels \n",
    "                            of shape samples x 1\n",
    "        keys_list:          None or list of keys corresponding to each sensor node\n",
    "                            of shape sensors x 1\n",
    "                            and in the order of the corresponding thresholds \n",
    "                            in the thresholds_array\n",
    "                            (if None, self.thresholds.keys() is used)\n",
    "        thresholds_array:   None or numpy array of thresholds used for each sensor node\n",
    "                            of shape nodes x 1\n",
    "        print_all_scores:   boolean which indicates whether scores\n",
    "                            should be printed or not\n",
    "                            \n",
    "        Output:\n",
    "        TPRs_list:          dict with key:sensitive group, \n",
    "                            value:TPR over all samples \n",
    "        eo:                 float of equal opportunity score\n",
    "        di:                 float of disparate impact score\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- compute TPR per sensitive group\n",
    "        sensitive_features = list(X_sen_test.columns)\n",
    "        TPRs_dict = dict()\n",
    "        for sensitive_feature in sensitive_features:\n",
    "\n",
    "            filter_group = X_sen_test[sensitive_feature] == 1\n",
    "            tpr,_,_,_,_,_,_ = self.score(X_test[filter_group], \n",
    "                                         y_test[filter_group],\n",
    "                                         keys_list=keys_list,\n",
    "                                         thresholds_array=thresholds_array)\n",
    "            TPRs_dict[sensitive_feature] = tpr\n",
    "\n",
    "        # --- compute fairness scores\n",
    "        TPRs_list = list(TPRs_dict.values())\n",
    "        eo = max(TPRs_list) - min(TPRs_list)\n",
    "        di = min(TPRs_list)/max(TPRs_list)\n",
    "\n",
    "        if print_all_scores:\n",
    "            print('TPRs:\\n', TPRs_dict)\n",
    "            print('Equal opportunity score (< epsilon), e.g. < 0.2:\\n', eo)\n",
    "            print('Disparate impact score (> 1 - epsilon), e.g. > 0.8:\\n', di)\n",
    "            # note that (1 - di) * max(TPRs_list) == eo\n",
    "\n",
    "        return TPRs_dict, eo, di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a412ea9-2aed-40e6-9c01-f939ae0bdc65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# threshold classification ensemle sub classes\n",
    "# (based on residual inputs)\n",
    "class ETC_hyperparameter(EnsembleThresholdClassification):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 alias='H',\n",
    "                 classifier=ThresholdClassification, \n",
    "                 classifier_approx=ThresholdClassificationApproximation):\n",
    "        super().__init__(classifier, classifier_approx)\n",
    "        self.alias = alias\n",
    "        \n",
    "    def fit(self, X_train, factor=1, print_coeff=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Train residual based threshold classification model (leak detector)\n",
    "        per sensor node\n",
    "        based on the (pressure) residual inputs per sensor nodes\n",
    "        by learning one threshold per sensor node\n",
    "        by choosing some procentual amount of the largest training error \n",
    "        per sensor node.\n",
    "        \n",
    "        Input:\n",
    "        X_train:      dataframe of training (pressure) residual inputs \n",
    "                      of shape samples x sensors\n",
    "        factor:       positive float that is the procentual amount \n",
    "                      of the largest training error that should be used\n",
    "        print_coeff:  boolean which indicates whether trained coefficients\n",
    "                      should be printed or not\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- train one classification model (leak detector) per sensor node\n",
    "        # --- by finding the largest training error per node\n",
    "        sensor_ids = list(X_train.columns)\n",
    "        for node in sensor_ids:\n",
    "\n",
    "            # -- train classification model (leak detector)\n",
    "            X_train_node = X_train.loc[:,node]\n",
    "            threshold = factor * X_train_node.max()\n",
    "            self.thresholds[node] = threshold\n",
    "            \n",
    "        # -- store trained classification model (leak detector) per sensor node\n",
    "        self.fit_model(self.thresholds, print_coeff=print_coeff)\n",
    "            \n",
    "class ETC_optimizeFTPR_db(EnsembleThresholdClassification):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 alias='T-F-PR',\n",
    "                 classifier=ThresholdClassification, \n",
    "                 classifier_approx=ThresholdClassificationApproximation):\n",
    "        super().__init__(classifier, classifier_approx)\n",
    "        self.alias = alias\n",
    "            \n",
    "    def fit(self, \n",
    "            X_train, \n",
    "            y_train,\n",
    "            start_thresholds,\n",
    "            b_sigmoid=100, \n",
    "            sum_threshold=0.8,\n",
    "            print_coeff=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Train residual based threshold classification model (leak detector) \n",
    "        per sensor node\n",
    "        based on the (pressure) residual inputs per sensor nodes\n",
    "        by learning one threshold per sensor node\n",
    "        by minimizing the db. approximation of the objective F = -TPR + FPR \n",
    "        over these thresholds.\n",
    "        \n",
    "        Input:\n",
    "        X_train:            dataframe of training (pressure) residual inputs \n",
    "                            of shape samples x sensors\n",
    "        y_train:            dataframe of training (leak y/n) labels \n",
    "                            of shape samples x 1\n",
    "        start_thresholds:   numpy array of thresholds used for each sensor node\n",
    "                            of shape nodes x 1\n",
    "        b_sigmoid:          positive float that indicates \n",
    "                            the b in the sigmoid function\n",
    "        sum_thresholds:     positive float that indicates\n",
    "                            when the sum of sensor node outputs indicates a leak\n",
    "        print_coeff:        boolean which indicates whether trained coefficients\n",
    "                            should be printed or not\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- train one classification model (leak detector) per sensor node\n",
    "        # --- by minimizing an objective of the ensemble classification model\n",
    "        \n",
    "        # -- define objective and objective's gradient\n",
    "        def F(thresholds_array):\n",
    "            _, y_pred = self.predict_approx(X_train, \n",
    "                                            keys_list=list(start_thresholds.keys()),\n",
    "                                            thresholds_array=thresholds_array, \n",
    "                                            b_sigmoid=b_sigmoid,\n",
    "                                            sum_threshold=sum_threshold)\n",
    "            F = -TPR(y_train,y_pred) + FPR(y_train,y_pred)\n",
    "            if print_coeff:\n",
    "                print('\\nF with no constraint:', F)\n",
    "            return np.array(F)\n",
    "            \n",
    "        def gradF(thresholds_array):\n",
    "            dx_Y_pred = self.dx_predict_approx(X_train,\n",
    "                                               keys_list=list(start_thresholds.keys()),\n",
    "                                               thresholds_array=thresholds_array, \n",
    "                                               b_sigmoid=b_sigmoid,\n",
    "                                               sum_threshold=sum_threshold)\n",
    "            gradF = list()\n",
    "            sensor_ids = list(X_train.columns)\n",
    "            for node in sensor_ids:\n",
    "                dx_y_pred = dx_Y_pred.loc[:,node]\n",
    "                dx_y_pred = pd.DataFrame(data=dx_y_pred,\n",
    "                                         index=dx_y_pred.index)\n",
    "                dx_y_pred = dx_y_pred.rename({node:'y'},\n",
    "                                              axis='columns')\n",
    "                partialF = -TPR(y_train,dx_y_pred) + FPR(y_train,dx_y_pred)\n",
    "                gradF.append(partialF)\n",
    "            if print_coeff:    \n",
    "                print('\\ngradF with no constraint:', gradF)\n",
    "            return np.array(gradF)\n",
    "        \n",
    "        # -- run minimization of objective\n",
    "        start_thresholds_array = list(start_thresholds.values())\n",
    "        result = minimize(fun=F, \n",
    "                          x0=start_thresholds_array, \n",
    "                          jac=gradF, \n",
    "                          method=\"BFGS\")\n",
    "        print(result.message)\n",
    "        thresholds_array = result.x\n",
    "        self.thresholds = dict(zip(start_thresholds.keys(), thresholds_array))\n",
    "    \n",
    "        # -- store trained classification model (leak detector) per sensor node\n",
    "        self.fit_model(self.thresholds, print_coeff=print_coeff)\n",
    "            \n",
    "            \n",
    "class ETC_optimizeFTPR_F_db(EnsembleThresholdClassification):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 alias='T-F-PR+F',\n",
    "                 classifier=ThresholdClassification, \n",
    "                 classifier_approx=ThresholdClassificationApproximation):\n",
    "        super().__init__(classifier, classifier_approx)\n",
    "        self.alias = alias\n",
    "            \n",
    "    def fit(self, \n",
    "            X_train,\n",
    "            X_sen_train,\n",
    "            y_train,\n",
    "            start_thresholds,\n",
    "            b_sigmoid=100, \n",
    "            sum_threshold=0.8,\n",
    "            c=0.5,\n",
    "            mu=0.1,\n",
    "            print_coeff=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Train residual based threshold classification model (leak detector) \n",
    "        per sensor node\n",
    "        based on the (pressure) residual inputs per sensor nodes\n",
    "        by learning one threshold per sensor node\n",
    "        by minimizing the db. approximation of the objective F = -TPR + FPR \n",
    "        over these thresholds.\n",
    "        under some db. approximation covariance based side constraints, \n",
    "        put into a log barrier penalty.\n",
    "        \n",
    "        Input:\n",
    "        X_train:            dataframe of training (pressure) residual inputs \n",
    "                            of shape samples x sensors\n",
    "        X_sen_train         dataframe of test sensitive (feature) inputs \n",
    "                            of shape samples x sensitive features\n",
    "        y_train:            dataframe of training (leak y/n) labels \n",
    "                            of shape samples x 1\n",
    "        start_thresholds:   numpy array of thresholds used for each sensor node\n",
    "                            of shape nodes x 1\n",
    "        b_sigmoid:          positive float that indicates \n",
    "                            the b in the sigmoid function\n",
    "        sum_thresholds:     positive float that indicates\n",
    "                            when the sum of sensor node outputs indicates a leak\n",
    "        c:                  positive float that indicates\n",
    "                            the upper bound of the absolute value\n",
    "                            of the side constraint(s)\n",
    "        mu:                 positive float that indicates\n",
    "                            the hyperparameter of the log barrier method                    \n",
    "        print_coeff:        boolean which indicates whether trained coefficients\n",
    "                            should be printed or not\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- train one classification model (leak detector) per sensor node\n",
    "        # --- by minimizing an objective of the ensemble classification model\n",
    "        \n",
    "        # -- define objective and objective's gradient\n",
    "        def F(thresholds_array):\n",
    "            _, y_pred = self.predict_approx(X_train, \n",
    "                                            keys_list=list(start_thresholds.keys()),\n",
    "                                            thresholds_array=thresholds_array, \n",
    "                                            b_sigmoid=b_sigmoid,\n",
    "                                            sum_threshold=sum_threshold)\n",
    "            F = -TPR(y_train,y_pred) + FPR(y_train,y_pred)\n",
    "            if print_coeff:\n",
    "                print('\\nF with no constraint:', F)\n",
    "            \n",
    "            sensitive_features = list(X_sen_train.columns)\n",
    "            for sensitive_feature in sensitive_features:\n",
    "                x_sen_train = X_sen_train.loc[:, sensitive_feature]\n",
    "                cov = Cov(x_sen_train, y_pred.loc[:,'y'])\n",
    "                F -= mu * log_on_R(c - cov)\n",
    "                F -= mu * log_on_R(c + cov)\n",
    "                if print_coeff:\n",
    "                    print(sensitive_feature)\n",
    "                    print('cov:', cov)\n",
    "                    print('F', F)\n",
    "            \n",
    "            return np.array(F)\n",
    "            \n",
    "        def gradF(thresholds_array):\n",
    "            _, y_pred = self.predict_approx(X_train, \n",
    "                                            keys_list=list(start_thresholds.keys()),\n",
    "                                            thresholds_array=thresholds_array, \n",
    "                                            b_sigmoid=b_sigmoid,\n",
    "                                            sum_threshold=sum_threshold)\n",
    "            dx_Y_pred = self.dx_predict_approx(X_train,\n",
    "                                               keys_list=list(start_thresholds.keys()),\n",
    "                                               thresholds_array=thresholds_array, \n",
    "                                               b_sigmoid=b_sigmoid,\n",
    "                                               sum_threshold=sum_threshold)\n",
    "            gradF = list()\n",
    "            sensor_ids = list(X_train.columns)\n",
    "            for node in sensor_ids:\n",
    "                dx_y_pred = dx_Y_pred.loc[:,node]\n",
    "                dx_y_pred = pd.DataFrame(data=dx_y_pred,\n",
    "                                         index=dx_y_pred.index)\n",
    "                dx_y_pred = dx_y_pred.rename({node:'y'},\n",
    "                                              axis='columns')\n",
    "                partialF = -TPR(y_train,dx_y_pred) + FPR(y_train,dx_y_pred)\n",
    "                \n",
    "                sensitive_features = list(X_sen_train.columns)\n",
    "                for sensitive_feature in sensitive_features:\n",
    "                    x_sen_train = X_sen_train.loc[:, sensitive_feature]\n",
    "                    cov = Cov(x_sen_train, y_pred.loc[:,'y'])\n",
    "                    dx_cov = Cov(x_sen_train, dx_y_pred.loc[:,'y'])\n",
    "                    partialF -= (mu * -1 * dx_cov) / (c - cov)\n",
    "                    partialF -= (mu * dx_cov) / (c + cov)\n",
    "                gradF.append(partialF)\n",
    "                \n",
    "            if print_coeff:\n",
    "                print('\\ngradF with no constraint:', gradF)  \n",
    "            return np.array(gradF)\n",
    "        \n",
    "        # -- run minimization of objective\n",
    "        start_thresholds_array = list(start_thresholds.values())\n",
    "        result = minimize(fun=F, \n",
    "                          x0=start_thresholds_array, \n",
    "                          jac=gradF, \n",
    "                          method=\"BFGS\")\n",
    "        print(result.message)\n",
    "        thresholds_array = result.x\n",
    "        self.thresholds = dict(zip(start_thresholds.keys(), thresholds_array))\n",
    "    \n",
    "        # -- store trained classification model (leak detector) per sensor node\n",
    "        self.fit_model(self.thresholds, print_coeff=print_coeff)\n",
    "        \n",
    "class ETC_optimizeFTPR_ndb(EnsembleThresholdClassification):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 alias='T-F-PR-ndb',\n",
    "                 classifier=ThresholdClassification, \n",
    "                 classifier_approx=ThresholdClassificationApproximation):\n",
    "        super().__init__(classifier, classifier_approx)\n",
    "        self.alias = alias\n",
    "            \n",
    "    def fit(self, \n",
    "            X_train, \n",
    "            y_train,\n",
    "            start_thresholds,\n",
    "            print_coeff=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Train residual based threshold classification model (leak detector) \n",
    "        per sensor node\n",
    "        based on the (pressure) residual inputs per sensor nodes\n",
    "        by learning one threshold per sensor node\n",
    "        by minimizing the exact objective F = -TPR + FPR \n",
    "        over these thresholds.\n",
    "        \n",
    "        Input:\n",
    "        X_train:            dataframe of training (pressure) residual inputs \n",
    "                            of shape samples x sensors\n",
    "        y_train:            dataframe of training (leak y/n) labels \n",
    "                            of shape samples x 1\n",
    "        start_thresholds:   numpy array of thresholds used for each sensor node\n",
    "                            of shape nodes x 1\n",
    "        print_coeff:        boolean which indicates whether trained coefficients\n",
    "                            should be printed or not\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- train one classification model (leak detector) per sensor node\n",
    "        # --- by minimizing an objective of the ensemble classification model\n",
    "        \n",
    "        # -- define objective \n",
    "        def F(thresholds_array):\n",
    "            _, y_pred = self.predict(X_train, \n",
    "                                     keys_list=list(start_thresholds.keys()),\n",
    "                                     thresholds_array=thresholds_array)\n",
    "            F = -TPR(y_train,y_pred) + FPR(y_train,y_pred)\n",
    "            if print_coeff:\n",
    "                print('\\nF with no constraint:', F)\n",
    "            return np.array(F)\n",
    "        \n",
    "        # -- run minimization of objective\n",
    "        start_thresholds_array = list(start_thresholds.values())\n",
    "        result = minimize(fun=F, \n",
    "                          x0=start_thresholds_array,  \n",
    "                          method=\"Nelder-Mead\")\n",
    "        print(result.message)\n",
    "        thresholds_array = result.x\n",
    "        self.thresholds = dict(zip(start_thresholds.keys(), thresholds_array))\n",
    "    \n",
    "        # -- store trained classification model (leak detector) per sensor node\n",
    "        self.fit_model(self.thresholds, print_coeff=print_coeff)\n",
    "        \n",
    "class ETC_optimizeFTPR_F_ndb(EnsembleThresholdClassification):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 alias='T-F-PR+F-ndb',\n",
    "                 classifier=ThresholdClassification, \n",
    "                 classifier_approx=ThresholdClassificationApproximation):\n",
    "        super().__init__(classifier, classifier_approx)\n",
    "        self.alias = alias\n",
    "            \n",
    "    def fit(self, \n",
    "            X_train,\n",
    "            X_sen_train,\n",
    "            y_train,\n",
    "            start_thresholds,\n",
    "            c=0.5,\n",
    "            mu=0.1,\n",
    "            print_coeff=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Train residual based threshold classification model (leak detector) \n",
    "        per sensor node\n",
    "        based on the (pressure) residual inputs per sensor nodes\n",
    "        by learning one threshold per sensor node\n",
    "        by minimizing the exact objective F = -TPR + FPR \n",
    "        over these thresholds\n",
    "        under some exact covariance based side constraints,\n",
    "        put into a log barrier penalty.\n",
    "        \n",
    "        Input:\n",
    "        X_train:            dataframe of training (pressure) residual inputs \n",
    "                            of shape samples x sensors\n",
    "        X_sen_train         dataframe of test sensitive (feature) inputs \n",
    "                            of shape samples x sensitive features\n",
    "        y_train:            dataframe of training (leak y/n) labels \n",
    "                            of shape samples x 1\n",
    "        start_thresholds:   numpy array of thresholds used for each sensor node\n",
    "                            of shape nodes x 1\n",
    "        c:                  positive float that indicates\n",
    "                            the upper bound of the absolute value\n",
    "                            of the side constraint(s)\n",
    "        mu:                 positive float that indicates\n",
    "                            the hyperparameter of the log barrier method                    \n",
    "        print_coeff:        boolean which indicates whether trained coefficients\n",
    "                            should be printed or not\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- train one classification model (leak detector) per sensor node\n",
    "        # --- by minimizing an objective of the ensemble classification model\n",
    "        \n",
    "        # -- define objective\n",
    "        def F(thresholds_array):\n",
    "            _, y_pred = self.predict(X_train, \n",
    "                                     keys_list=list(start_thresholds.keys()),\n",
    "                                     thresholds_array=thresholds_array)\n",
    "            F = -TPR(y_train,y_pred) + FPR(y_train,y_pred)\n",
    "            if print_coeff:\n",
    "                print('\\nF with no constraint:', F)\n",
    "            \n",
    "            sensitive_features = list(X_sen_train.columns)\n",
    "            for sensitive_feature in sensitive_features:\n",
    "                x_sen_train = X_sen_train.loc[:, sensitive_feature]\n",
    "                cov = Cov(x_sen_train, y_pred.loc[:,'y'])\n",
    "                F -= mu * log_on_R(c - cov)\n",
    "                F -= mu * log_on_R(c + cov)\n",
    "                if print_coeff:\n",
    "                    print(sensitive_feature)\n",
    "                    print('cov:', cov)\n",
    "                    print('F', F)\n",
    "            \n",
    "            return np.array(F)\n",
    "        \n",
    "        # -- run minimization of objective\n",
    "        start_thresholds_array = list(start_thresholds.values())\n",
    "        result = minimize(fun=F, \n",
    "                          x0=start_thresholds_array,  \n",
    "                          method=\"Nelder-Mead\")\n",
    "        print(result.message)\n",
    "        thresholds_array = result.x\n",
    "        self.thresholds = dict(zip(start_thresholds.keys(), thresholds_array))\n",
    "    \n",
    "        # -- store trained classification model (leak detector) per sensor node\n",
    "        self.fit_model(self.thresholds, print_coeff=print_coeff)\n",
    "        \n",
    "class ETC_optimizeACC_db(EnsembleThresholdClassification):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 alias='ACC',\n",
    "                 classifier=ThresholdClassification, \n",
    "                 classifier_approx=ThresholdClassificationApproximation):\n",
    "        super().__init__(classifier, classifier_approx)\n",
    "        self.alias = alias\n",
    "            \n",
    "    def fit(self, \n",
    "            X_train, \n",
    "            y_train,\n",
    "            start_thresholds,\n",
    "            b_sigmoid=100, \n",
    "            sum_threshold=0.8,\n",
    "            print_coeff=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Train residual based threshold classification model (leak detector) \n",
    "        per sensor node\n",
    "        based on the (pressure) residual inputs per sensor nodes\n",
    "        by learning one threshold per sensor node\n",
    "        by minimizing the db. approximation of the objective F = -AUC\n",
    "        over these thresholds.\n",
    "        \n",
    "        Input:\n",
    "        X_train:            dataframe of training (pressure) residual inputs \n",
    "                            of shape samples x sensors\n",
    "        y_train:            dataframe of training (leak y/n) labels \n",
    "                            of shape samples x 1\n",
    "        start_thresholds:   numpy array of thresholds used for each sensor node\n",
    "                            of shape nodes x 1\n",
    "        b_sigmoid:          positive float that indicates \n",
    "                            the b in the sigmoid function\n",
    "        sum_thresholds:     positive float that indicates\n",
    "                            when the sum of sensor node outputs indicates a leak\n",
    "        print_coeff:        boolean which indicates whether trained coefficients\n",
    "                            should be printed or not\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- train one classification model (leak detector) per sensor node\n",
    "        # --- by minimizing an objective of the ensemble classification model\n",
    "        \n",
    "        # -- define objective and objective's gradient\n",
    "        def F(thresholds_array):\n",
    "            _, y_pred = self.predict_approx(X_train, \n",
    "                                            keys_list=list(start_thresholds.keys()),\n",
    "                                            thresholds_array=thresholds_array, \n",
    "                                            b_sigmoid=b_sigmoid,\n",
    "                                            sum_threshold=sum_threshold)\n",
    "            F = -ACC(y_train,y_pred)\n",
    "            if print_coeff:\n",
    "                print('\\nF with no constraint:', F)\n",
    "            return np.array(F)\n",
    "            \n",
    "        def gradF(thresholds_array):\n",
    "            dx_Y_pred = self.dx_predict_approx(X_train,\n",
    "                                               keys_list=list(start_thresholds.keys()),\n",
    "                                               thresholds_array=thresholds_array, \n",
    "                                               b_sigmoid=b_sigmoid,\n",
    "                                               sum_threshold=sum_threshold)\n",
    "            gradF = list()\n",
    "            sensor_ids = list(X_train.columns)\n",
    "            for node in sensor_ids:\n",
    "                dx_y_pred = dx_Y_pred.loc[:,node]\n",
    "                dx_y_pred = pd.DataFrame(data=dx_y_pred,\n",
    "                                         index=dx_y_pred.index)\n",
    "                dx_y_pred = dx_y_pred.rename({node:'y'},\n",
    "                                              axis='columns')\n",
    "                partialF = -dx_ACC(y_train,dx_y_pred)\n",
    "                gradF.append(partialF)\n",
    "            if print_coeff:\n",
    "                print('\\ngradF with no constraint:', gradF)    \n",
    "            return np.array(gradF)\n",
    "        \n",
    "        # -- run minimization of objective\n",
    "        start_thresholds_array = list(start_thresholds.values())\n",
    "        result = minimize(fun=F, \n",
    "                          x0=start_thresholds_array, \n",
    "                          jac=gradF, \n",
    "                          method=\"BFGS\")\n",
    "        print(result.message)\n",
    "        thresholds_array = result.x\n",
    "        self.thresholds = dict(zip(start_thresholds.keys(), thresholds_array))\n",
    "    \n",
    "        # -- store trained classification model (leak detector) per sensor node\n",
    "        self.fit_model(self.thresholds, print_coeff=print_coeff)\n",
    "            \n",
    "            \n",
    "class ETC_optimizeACC_F_db(EnsembleThresholdClassification):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 alias='ACC+F',\n",
    "                 classifier=ThresholdClassification, \n",
    "                 classifier_approx=ThresholdClassificationApproximation):\n",
    "        super().__init__(classifier, classifier_approx)\n",
    "        self.alias = alias\n",
    "            \n",
    "    def fit(self, \n",
    "            X_train,\n",
    "            X_sen_train,\n",
    "            y_train,\n",
    "            start_thresholds,\n",
    "            b_sigmoid=100, \n",
    "            sum_threshold=0.8,\n",
    "            c=0.5,\n",
    "            mu=0.1,\n",
    "            print_coeff=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Train residual based threshold classification model (leak detector) \n",
    "        per sensor node\n",
    "        based on the (pressure) residual inputs per sensor nodes\n",
    "        by learning one threshold per sensor node\n",
    "        by minimizing the db. approximation of the objective F = -AUC\n",
    "        over these thresholds\n",
    "        under some db. approximation covariance based side constraints, \n",
    "        put into a log barrier penalty.\n",
    "        \n",
    "        Input:\n",
    "        X_train:            dataframe of training (pressure) residual inputs \n",
    "                            of shape samples x sensors\n",
    "        X_sen_train         dataframe of test sensitive (feature) inputs \n",
    "                            of shape samples x sensitive features\n",
    "        y_train:            dataframe of training (leak y/n) labels \n",
    "                            of shape samples x 1\n",
    "        start_thresholds:   numpy array of thresholds used for each sensor node\n",
    "                            of shape nodes x 1\n",
    "        b_sigmoid:          positive float that indicates \n",
    "                            the b in the sigmoid function\n",
    "        sum_thresholds:     positive float that indicates\n",
    "                            when the sum of sensor node outputs indicates a leak\n",
    "        c:                  positive float that indicates\n",
    "                            the upper bound of the absolute value\n",
    "                            of the side constraint(s)\n",
    "        mu:                 positive float that indicates\n",
    "                            the hyperparameter of the log barrier method                    \n",
    "        print_coeff:        boolean which indicates whether trained coefficients\n",
    "                            should be printed or not\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- train one classification model (leak detector) per sensor node\n",
    "        # --- by minimizing an objective of the ensemble classification model\n",
    "        \n",
    "        # -- define objective and objective's gradient\n",
    "        def F(thresholds_array):\n",
    "            _, y_pred = self.predict_approx(X_train, \n",
    "                                            keys_list=list(start_thresholds.keys()),\n",
    "                                            thresholds_array=thresholds_array, \n",
    "                                            b_sigmoid=b_sigmoid,\n",
    "                                            sum_threshold=sum_threshold)\n",
    "            F = -ACC(y_train,y_pred)\n",
    "            if print_coeff:\n",
    "                print('\\nF with no constraint:', F)\n",
    "            \n",
    "            sensitive_features = list(X_sen_train.columns)\n",
    "            for sensitive_feature in sensitive_features:\n",
    "                x_sen_train = X_sen_train.loc[:, sensitive_feature]\n",
    "                cov = Cov(x_sen_train, y_pred.loc[:,'y'])\n",
    "                F -= mu * log_on_R(c - cov)\n",
    "                F -= mu * log_on_R(c + cov)\n",
    "                if print_coeff:\n",
    "                    print(sensitive_feature)\n",
    "                    print('cov:', cov)\n",
    "                    print('F', F)\n",
    "            \n",
    "            return np.array(F)\n",
    "            \n",
    "        def gradF(thresholds_array):\n",
    "            _, y_pred = self.predict_approx(X_train, \n",
    "                                            keys_list=list(start_thresholds.keys()),\n",
    "                                            thresholds_array=thresholds_array, \n",
    "                                            b_sigmoid=b_sigmoid,\n",
    "                                            sum_threshold=sum_threshold)\n",
    "            dx_Y_pred = self.dx_predict_approx(X_train,\n",
    "                                               keys_list=list(start_thresholds.keys()),\n",
    "                                               thresholds_array=thresholds_array, \n",
    "                                               b_sigmoid=b_sigmoid,\n",
    "                                               sum_threshold=sum_threshold)\n",
    "            gradF = list()\n",
    "            sensor_ids = list(X_train.columns)\n",
    "            for node in sensor_ids:\n",
    "                dx_y_pred = dx_Y_pred.loc[:,node]\n",
    "                dx_y_pred = pd.DataFrame(data=dx_y_pred,\n",
    "                                         index=dx_y_pred.index)\n",
    "                dx_y_pred = dx_y_pred.rename({node:'y'},\n",
    "                                              axis='columns')\n",
    "                partialF = -dx_ACC(y_train,dx_y_pred)\n",
    "                \n",
    "                sensitive_features = list(X_sen_train.columns)\n",
    "                for sensitive_feature in sensitive_features:\n",
    "                    x_sen_train = X_sen_train.loc[:, sensitive_feature]\n",
    "                    cov = Cov(x_sen_train, y_pred.loc[:,'y'])\n",
    "                    dx_cov = Cov(x_sen_train, dx_y_pred.loc[:,'y'])\n",
    "                    partialF -= (mu * -1 * dx_cov) / (c - cov)\n",
    "                    partialF -= (mu * dx_cov) / (c + cov)\n",
    "                gradF.append(partialF)\n",
    "                \n",
    "            if print_coeff:    \n",
    "                print('\\ngradF with no constraint:', gradF)       \n",
    "            return np.array(gradF)\n",
    "        \n",
    "        # -- run minimization of objective\n",
    "        start_thresholds_array = list(start_thresholds.values())\n",
    "        result = minimize(fun=F, \n",
    "                          x0=start_thresholds_array, \n",
    "                          jac=gradF, \n",
    "                          method=\"BFGS\")\n",
    "        print(result.message)\n",
    "        thresholds_array = result.x\n",
    "        self.thresholds = dict(zip(start_thresholds.keys(), thresholds_array))\n",
    "    \n",
    "        # -- store trained classification model (leak detector) per sensor node\n",
    "        self.fit_model(self.thresholds, print_coeff=print_coeff)\n",
    "        \n",
    "class ETC_optimizeACC_ndb(EnsembleThresholdClassification):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 alias='ACC-ndb',\n",
    "                 classifier=ThresholdClassification, \n",
    "                 classifier_approx=ThresholdClassificationApproximation):\n",
    "        super().__init__(classifier, classifier_approx)\n",
    "        self.alias = alias\n",
    "            \n",
    "    def fit(self, \n",
    "            X_train, \n",
    "            y_train,\n",
    "            start_thresholds,\n",
    "            print_coeff=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Train residual based threshold classification model (leak detector) \n",
    "        per sensor node\n",
    "        based on the (pressure) residual inputs per sensor nodes\n",
    "        by learning one threshold per sensor node\n",
    "        by minimizing the exact objective F = -AUC\n",
    "        over these thresholds.\n",
    "        \n",
    "        Input:\n",
    "        X_train:            dataframe of training (pressure) residual inputs \n",
    "                            of shape samples x sensors\n",
    "        y_train:            dataframe of training (leak y/n) labels \n",
    "                            of shape samples x 1\n",
    "        start_thresholds:   numpy array of thresholds used for each sensor node\n",
    "                            of shape nodes x 1\n",
    "        print_coeff:        boolean which indicates whether trained coefficients\n",
    "                            should be printed or not\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- train one classification model (leak detector) per sensor node\n",
    "        # --- by minimizing an objective of the ensemble classification model\n",
    "        \n",
    "        # -- define objective\n",
    "        def F(thresholds_array):\n",
    "            _, y_pred = self.predict(X_train, \n",
    "                                     keys_list=list(start_thresholds.keys()),\n",
    "                                     thresholds_array=thresholds_array)\n",
    "            F = -ACC(y_train,y_pred)\n",
    "            if print_coeff:\n",
    "                print('\\nF with no constraint:', F)\n",
    "            return np.array(F)\n",
    "        \n",
    "        # -- run minimization of objective\n",
    "        start_thresholds_array = list(start_thresholds.values())\n",
    "        result = minimize(fun=F, \n",
    "                          x0=start_thresholds_array,  \n",
    "                          method=\"Nelder-Mead\")\n",
    "        print(result.message)\n",
    "        thresholds_array = result.x\n",
    "        self.thresholds = dict(zip(start_thresholds.keys(), thresholds_array))\n",
    "    \n",
    "        # -- store trained classification model (leak detector) per sensor node\n",
    "        self.fit_model(self.thresholds, print_coeff=print_coeff)\n",
    "        \n",
    "class ETC_optimizeACC_F_ndb(EnsembleThresholdClassification):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 alias='ACC+F-ndb',\n",
    "                 classifier=ThresholdClassification, \n",
    "                 classifier_approx=ThresholdClassificationApproximation):\n",
    "        super().__init__(classifier, classifier_approx)\n",
    "        self.alias = alias\n",
    "            \n",
    "    def fit(self, \n",
    "            X_train,\n",
    "            X_sen_train,\n",
    "            y_train,\n",
    "            start_thresholds,\n",
    "            c=0.5,\n",
    "            mu=0.1,\n",
    "            print_coeff=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Train residual based threshold classification model (leak detector) \n",
    "        per sensor node\n",
    "        based on the (pressure) residual inputs per sensor nodes\n",
    "        by learning one threshold per sensor node\n",
    "        by minimizing the exact objective F = -AUC\n",
    "        over these thresholds\n",
    "        under some exact covariance based side constraints,\n",
    "        put into a log barrier penalty.\n",
    "        \n",
    "        Input:\n",
    "        X_train:            dataframe of training (pressure) residual inputs \n",
    "                            of shape samples x sensors\n",
    "        X_sen_train         dataframe of test sensitive (feature) inputs \n",
    "                            of shape samples x sensitive features\n",
    "        y_train:            dataframe of training (leak y/n) labels \n",
    "                            of shape samples x 1\n",
    "        start_thresholds:   numpy array of thresholds used for each sensor node\n",
    "                            of shape nodes x 1\n",
    "        c:                  positive float that indicates\n",
    "                            the upper bound of the absolute value\n",
    "                            of the side constraint(s)\n",
    "        mu:                 positive float that indicates\n",
    "                            the hyperparameter of the log barrier method                    \n",
    "        print_coeff:        boolean which indicates whether trained coefficients\n",
    "                            should be printed or not\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- train one classification model (leak detector) per sensor node\n",
    "        # --- by minimizing an objective of the ensemble classification model\n",
    "        \n",
    "        # -- define objective\n",
    "        def F(thresholds_array):\n",
    "            _, y_pred = self.predict(X_train, \n",
    "                                     keys_list=list(start_thresholds.keys()),\n",
    "                                     thresholds_array=thresholds_array)\n",
    "            F = -ACC(y_train,y_pred)\n",
    "            if print_coeff:\n",
    "                print('\\nF with no constraint:', F)\n",
    "            \n",
    "            sensitive_features = list(X_sen_train.columns)\n",
    "            for sensitive_feature in sensitive_features:\n",
    "                x_sen_train = X_sen_train.loc[:, sensitive_feature]\n",
    "                cov = Cov(x_sen_train, y_pred.loc[:,'y'])\n",
    "                F -= mu * log_on_R(c - cov)\n",
    "                F -= mu * log_on_R(c + cov)\n",
    "                if print_coeff:\n",
    "                    print(sensitive_feature)\n",
    "                    print('cov:', cov)\n",
    "                    print('F', F)\n",
    "            \n",
    "            return np.array(F)\n",
    "        \n",
    "        # -- run minimization of objective\n",
    "        start_thresholds_array = list(start_thresholds.values())\n",
    "        result = minimize(fun=F, \n",
    "                          x0=start_thresholds_array,  \n",
    "                          method=\"Nelder-Mead\")\n",
    "        print(result.message)\n",
    "        thresholds_array = result.x\n",
    "        self.thresholds = dict(zip(start_thresholds.keys(), thresholds_array))\n",
    "    \n",
    "        # -- store trained classification model (leak detector) per sensor node\n",
    "        self.fit_model(self.thresholds, print_coeff=print_coeff)\n",
    "        \n",
    "class ETC_optimizeEO_ndb(EnsembleThresholdClassification):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 alias='EO+ACC-ndb',\n",
    "                 classifier=ThresholdClassification, \n",
    "                 classifier_approx=ThresholdClassificationApproximation):\n",
    "        super().__init__(classifier, classifier_approx)\n",
    "        self.alias = alias\n",
    "        \n",
    "    def fit(self, \n",
    "            X_train,\n",
    "            X_sen_train,\n",
    "            y_train,\n",
    "            start_thresholds,\n",
    "            mu=0.1,\n",
    "            lamb=0.1,\n",
    "            acc_best=1,\n",
    "            print_coeff=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Train residual based threshold classification model (leak detector) \n",
    "        per sensor node\n",
    "        based on the (pressure) residual inputs per sensor nodes\n",
    "        by learning one threshold per sensor node\n",
    "        by minimizing the exact objective F = EO\n",
    "        over these thresholds\n",
    "        under some exact accuracy based side constraints,\n",
    "        put into a log barrier penalty.\n",
    "        \n",
    "        Input:\n",
    "        X_train:            dataframe of training (pressure) residual inputs \n",
    "                            of shape samples x sensors\n",
    "        X_sen_train         dataframe of test sensitive (feature) inputs \n",
    "                            of shape samples x sensitive features\n",
    "        y_train:            dataframe of training (leak y/n) labels \n",
    "                            of shape samples x 1\n",
    "        start_thresholds:   numpy array of thresholds used for each sensor node\n",
    "                            of shape nodes x 1\n",
    "        mu:                 positive float that indicates\n",
    "                            the hyperparameter of the log barrier method\n",
    "        lamb:                positive fliat that indicates\n",
    "                            the percentage we allow to deviate from acc_best\n",
    "        acc_best:           positive float between 0 and 1 that indicates\n",
    "                            the best accuarcy reach in this optimization problem\n",
    "                            without considering fairness constraints\n",
    "        print_coeff:        boolean which indicates whether trained coefficients\n",
    "                            should be printed or not\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- train one classification model (leak detector) per sensor node\n",
    "        # --- by minimizing an objective of the ensemble classification model\n",
    "        \n",
    "        # -- define objective \n",
    "        def F(thresholds_array):\n",
    "            _, y_pred = self.predict(X_train, \n",
    "                                     keys_list=\n",
    "                                     list(start_thresholds.keys()),\n",
    "                                     thresholds_array=thresholds_array)\n",
    "            acc = ACC(y_train,y_pred)\n",
    "            _,eo,_ = self.score_fairness(X_train,\n",
    "                                         X_sen_train,\n",
    "                                         y_train,\n",
    "                                         keys_list=list(start_thresholds.keys()),\n",
    "                                         thresholds_array=thresholds_array)\n",
    "            F = eo - mu * log_on_R(acc - (1-lamb)*acc_best)\n",
    "            if print_coeff:\n",
    "                print('\\nF:', F)\n",
    "            return np.array(F)\n",
    "        \n",
    "        # -- run minimization of objective\n",
    "        start_thresholds_array = list(start_thresholds.values())\n",
    "        result = minimize(fun=F, \n",
    "                          x0=start_thresholds_array,  \n",
    "                          method=\"Nelder-Mead\")\n",
    "        print(result.message)\n",
    "        thresholds_array = result.x\n",
    "        self.thresholds = dict(zip(start_thresholds.keys(), thresholds_array))\n",
    "    \n",
    "        # -- store trained classification model (leak detector) per sensor node\n",
    "        self.fit_model(self.thresholds, print_coeff=print_coeff)\n",
    "        \n",
    "class ETC_optimizeDI_ndb(EnsembleThresholdClassification):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 alias='DI+ACC',\n",
    "                 classifier=ThresholdClassification, \n",
    "                 classifier_approx=ThresholdClassificationApproximation):\n",
    "        super().__init__(classifier, classifier_approx)\n",
    "        self.alias = alias\n",
    "        \n",
    "    def fit(self, \n",
    "            X_train,\n",
    "            X_sen_train,\n",
    "            y_train,\n",
    "            start_thresholds,\n",
    "            mu=0.05,\n",
    "            lamb=0.04,\n",
    "            acc_best=1,\n",
    "            print_coeff=False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Train residual based threshold classification model (leak detector) \n",
    "        per sensor node\n",
    "        based on the (pressure) residual inputs per sensor nodes\n",
    "        by learning one threshold per sensor node\n",
    "        by minimizing the exact objective F = -DI\n",
    "        over these thresholds\n",
    "        under some exact accuracy based side constraints,\n",
    "        put into a log barrier penalty.\n",
    "        \n",
    "        Input:\n",
    "        X_train:            dataframe of training (pressure) residual inputs \n",
    "                            of shape samples x sensors\n",
    "        X_sen_train         dataframe of test sensitive (feature) inputs \n",
    "                            of shape samples x sensitive features\n",
    "        y_train:            dataframe of training (leak y/n) labels \n",
    "                            of shape samples x 1\n",
    "        start_thresholds:   numpy array of thresholds used for each sensor node\n",
    "                            of shape nodes x 1\n",
    "        mu:                 positive float that indicates\n",
    "                            the hyperparameter of the log barrier method\n",
    "        lamb:                positive fliat that indicates\n",
    "                            the percentage we allow to deviate from acc_best\n",
    "        acc_best:           positive float between 0 and 1 that indicates\n",
    "                            the best accuarcy reach in this optimization problem\n",
    "                            without considering fairness constraints\n",
    "        print_coeff:        boolean which indicates whether trained coefficients\n",
    "                            should be printed or not\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- train one classification model (leak detector) per sensor node\n",
    "        # --- by minimizing an objective of the ensemble classification model\n",
    "        \n",
    "        # -- define objective\n",
    "        def F(thresholds_array):\n",
    "            _, y_pred = self.predict(X_train, \n",
    "                                     keys_list=\n",
    "                                     list(start_thresholds.keys()),\n",
    "                                     thresholds_array=thresholds_array)\n",
    "            acc = ACC(y_train,y_pred)\n",
    "            _,_,di = self.score_fairness(X_train,\n",
    "                                         X_sen_train,\n",
    "                                         y_train,\n",
    "                                         keys_list=list(start_thresholds.keys()),\n",
    "                                         thresholds_array=thresholds_array)\n",
    "            F = - di - mu * log_on_R(acc - (1-lamb)*acc_best)\n",
    "            if print_coeff:\n",
    "                print('\\nF:', F)\n",
    "            return np.array(F)\n",
    "        \n",
    "        # -- run minimization of objective\n",
    "        start_thresholds_array = list(start_thresholds.values())\n",
    "        result = minimize(fun=F, \n",
    "                          x0=start_thresholds_array,  \n",
    "                          method=\"Nelder-Mead\")\n",
    "        print(result.message)\n",
    "        thresholds_array = result.x\n",
    "        self.thresholds = dict(zip(start_thresholds.keys(), thresholds_array))\n",
    "    \n",
    "        # -- store trained classification model (leak detector) per sensor node\n",
    "        self.fit_model(self.thresholds, print_coeff=print_coeff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84f4efe-122d-4126-b7ff-9761a1494b7b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluation and Visualization of Results\n",
    "\n",
    "- functionality to filter data according to leak size\n",
    "- functionality to evaluate a trained multi classification resp. ensemble classification model with respect to performance and fairness\n",
    "- functionality to visualize the performance of different trained multi classification resp. ensemble classification model with respect to performance and fairness\n",
    "- functionality to visualize the dependency on the performance score on the fairness scores\n",
    "- functionality to visualize the dependency on the performance and fairness scores on the training hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccef62e-ebca-4186-9731-a11e6d265f59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_diameter(X_clas,\n",
    "                    y_clas,\n",
    "                    X_sen,\n",
    "                    diameter,\n",
    "                    df_information,\n",
    "                    random_state=1):\n",
    "    \n",
    "    # find times where leak of diameter is fixed\n",
    "    filter_start = (df_information['node ID']=='2') & (df_information['diameter']==diameter)\n",
    "    filter_end = (df_information['node ID']=='32') & (df_information['diameter']==diameter)\n",
    "    index_start = list(df_information[filter_start].index)[0]\n",
    "    index_end = list(df_information[filter_end].index)[0]\n",
    "    time_start_diameter = df_information.loc[index_start, 'setting start ID']\n",
    "    time_end_diameter = df_information.loc[index_end, 'setting end ID'] \n",
    "\n",
    "    split = train_test_split(X_clas.loc[time_start_diameter:time_end_diameter,:], \n",
    "                             y_clas.loc[time_start_diameter:time_end_diameter,:], \n",
    "                             train_size=0.4,\n",
    "                             test_size=0.6, \n",
    "                             shuffle=True,\n",
    "                             random_state=random_state, \n",
    "                             stratify=y_clas.loc[time_start_diameter:time_end_diameter,:])\n",
    "    X_clas_train = split[0]\n",
    "    X_clas_test = split[1]\n",
    "    y_clas_train = split[2]\n",
    "    y_clas_test = split[3]\n",
    "    X_sen_train = X_sen.loc[X_clas_train.index,:]\n",
    "    X_sen_test = X_sen.loc[X_clas_test.index,:]\n",
    "    \n",
    "    return X_clas_train, X_clas_test, y_clas_train, y_clas_test, X_sen_train, X_sen_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd4b99a-0090-4bb7-8588-16a1f2fc21b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(model_clas):\n",
    "    \n",
    "    \"\"\"\n",
    "    Inputs:   \n",
    "    model_clas:   trained instance of one of the siblings of the \n",
    "                  EnsembleThresholdClassification class\n",
    "              \n",
    "    Outputs:\n",
    "    acc:          float of test accuracy score\n",
    "    eo:           float of test equal opportunity score\n",
    "    di:           float of test disparate impact score\n",
    "    TPRs:         dict with key:sensitive group, \n",
    "                  value:test TPR over all samples \n",
    "    \"\"\"\n",
    "    \n",
    "    tpr,_,fpr,_,_,acc,_ = model_clas.score(X_clas_train, \n",
    "                                           y_clas_train,\n",
    "                                           print_all_scores=False)\n",
    "    print('Train TPR:', tpr)\n",
    "    print('Train FPR:', fpr)\n",
    "    print('Train ACC:', acc)\n",
    "    \n",
    "    tpr,_,fpr,_,_,acc,_ = model_clas.score(X_clas_test, \n",
    "                                           y_clas_test,\n",
    "                                           print_all_scores=False)\n",
    "    print('Test TPR:', tpr)\n",
    "    print('Test FPR:', fpr)\n",
    "    print('Test ACC:', acc)\n",
    "    \n",
    "    TPRs,eo,di = model_clas.score_fairness(X_clas_test,\n",
    "                                      X_sen_test,\n",
    "                                      y_clas_test,\n",
    "                                      print_all_scores=False)\n",
    "    print('EO (small):', eo)\n",
    "    print('DI (large):', di)\n",
    "    \n",
    "    return acc, eo, di, TPRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599c5b27-d2e2-42f9-a1ff-2673e8e60224",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def graphics_bars(results,\n",
    "                  save_figs_d=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    results:     dictionary of the structure\n",
    "                 {model class: {'acc':  float of accuracy,\n",
    "                              'eo':   float of equal oppotunity,\n",
    "                              'di':   float of disparate impact,\n",
    "                              'TPRs': {sensitive feature: float of TPR of sensitive group\n",
    "                                      }\n",
    "                              }\n",
    "                 }\n",
    "    save_figs_d: string or float to determine whether and under which name\n",
    "                 the created grahps should be saved\n",
    "    \"\"\"\n",
    "\n",
    "    # --- create dataframes which will be used for plotting\n",
    "    columns1 = ['method','model','ACC', 'EO','DI','group', 'TPR']\n",
    "    df1 = pd.DataFrame(data=dict(), columns=columns1)\n",
    "    columns2 = ['method', 'score type', 'ACC resp. EO']\n",
    "    df2 = pd.DataFrame(data=dict(), columns=columns2)\n",
    "    columns3 = ['method', 'score type', 'ACC resp. DI']\n",
    "    df3 = pd.DataFrame(data=dict(), columns=columns3)\n",
    "    idx1, idx2, idx3 = 0, 0, 0\n",
    "    for model_clas in results:\n",
    "        \n",
    "        sensitive_features = results[model_clas]['TPRs'].keys()\n",
    "        for sensitive_feature in sensitive_features:\n",
    "            df1.loc[idx1,'method'] = model_clas.alias\n",
    "            df1.loc[idx1,'model'] = model_clas\n",
    "            df1.loc[idx1,'ACC'] = results[model_clas]['acc']\n",
    "            df1.loc[idx1,'EO'] = results[model_clas]['eo']\n",
    "            df1.loc[idx1,'DI'] = results[model_clas]['di']\n",
    "            df1.loc[idx1,'group'] = sensitive_feature[2:]\n",
    "            df1.loc[idx1,'TPR'] = results[model_clas]['TPRs'][sensitive_feature]\n",
    "            idx1 += 1\n",
    "            \n",
    "        df2.loc[idx2,'method'] = model_clas.alias\n",
    "        df2.loc[idx2,'score type'] = 'accuracy'\n",
    "        df2.loc[idx2,'ACC resp. EO'] = results[model_clas]['acc']\n",
    "        idx2 +=1\n",
    "        df2.loc[idx2,'method'] = model_clas.alias\n",
    "        df2.loc[idx2,'score type'] = 'equal opportunity'\n",
    "        df2.loc[idx2,'ACC resp. EO'] = results[model_clas]['eo']\n",
    "        idx2 +=1\n",
    "        \n",
    "        df3.loc[idx3,'method'] = model_clas.alias\n",
    "        df3.loc[idx3,'score type'] = 'accuracy'\n",
    "        df3.loc[idx3,'ACC resp. DI'] = results[model_clas]['acc']\n",
    "        idx3 +=1\n",
    "        df3.loc[idx3,'method'] = model_clas.alias\n",
    "        df3.loc[idx3,'score type'] = 'disparate impact'\n",
    "        df3.loc[idx3,'ACC resp. DI'] = results[model_clas]['di']\n",
    "        idx3 +=1\n",
    "    \n",
    "    # --- plot bar plots of TPR, ACC, EO and DI\n",
    "    # choose seaborn color palette\n",
    "    color_map = cm.get_cmap(name='Blues', lut=1000)\n",
    "    sub_color_map = color_map(np.linspace(0.2, 0.9, 3))\n",
    "    palette = sns.color_palette(palette=sub_color_map)\n",
    "    \n",
    "    # true positive rate per method and sensitive group\n",
    "    plt.figure(figsize=(5,3))\n",
    "    sns.barplot(x='method', y='TPR', \n",
    "                hue='group', \n",
    "                data=df1, \n",
    "                palette=palette,\n",
    "                estimator=lambda x: np.mean(x),\n",
    "                errorbar=lambda x: (x.min(),x.max()))\n",
    "    plt.legend(loc='lower left', bbox_to_anchor=(0.0, 0.0),\n",
    "              fontsize='medium')\n",
    "    plt.title('TPR per method and per sensitive group (d={})'.format(save_figs_d),\n",
    "              fontsize='medium')\n",
    "    #plt.xticks(rotation=10)\n",
    "    plt.tight_layout()\n",
    "    if save_figs_d!=None:\n",
    "        plt.savefig('TPRperMethodAndGroup_d{}.pdf'.format(save_figs_d),\n",
    "                    format='pdf')\n",
    "    plt.show()\n",
    "    \n",
    "    # accuracy and equal opportunity per method\n",
    "    plt.figure(figsize=(5,3))\n",
    "    sns.barplot(x='method', y='ACC resp. EO', \n",
    "                hue='score type', \n",
    "                data=df2, \n",
    "                palette=palette,\n",
    "                estimator=lambda x: np.mean(x),\n",
    "                errorbar=lambda x: (x.min(),x.max()))\n",
    "    plt.ylabel('accuracy and equal opportunity',\n",
    "              fontsize='medium')\n",
    "    plt.legend(loc='lower left', bbox_to_anchor=(0.0, 0.0),\n",
    "              fontsize='medium')\n",
    "    plt.title('Accuracy and equal opportunity per method (d={})'.format(save_figs_d),\n",
    "              fontsize='medium')\n",
    "    #plt.xticks(rotation=10)\n",
    "    plt.tight_layout()\n",
    "    if save_figs_d!=None:\n",
    "        plt.savefig('ACCandEOperMethod_d{}.pdf'.format(save_figs_d),\n",
    "                    format='pdf')\n",
    "    plt.show()\n",
    "    \n",
    "     # accuracy and disparate impact per method\n",
    "    fig = plt.figure(figsize=(5,3))\n",
    "    sns.barplot(x='method', y='ACC resp. DI', \n",
    "                hue='score type', \n",
    "                data=df3, \n",
    "                palette=palette,\n",
    "                estimator=lambda x: np.mean(x),\n",
    "                errorbar=lambda x: (x.min(),x.max()))\n",
    "    plt.ylabel('accuracy and disparate impact',\n",
    "              fontsize='medium')\n",
    "    plt.legend(loc='lower left', bbox_to_anchor=(0.0, 0.0),\n",
    "              fontsize='medium')\n",
    "    plt.title('Accuracy and disparate impact per method (d={})'.format(save_figs_d),\n",
    "              fontsize='medium')\n",
    "    #plt.xticks(rotation=10)\n",
    "    plt.tight_layout()\n",
    "    if save_figs_d!=None:\n",
    "        plt.savefig('ACCandDIperMethod_d{}.pdf'.format(save_figs_d),\n",
    "                    format='pdf')\n",
    "    plt.show()\n",
    "    \n",
    "    return df1, df2, df3, fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f806ec51-b039-4a14-8ff7-d0e204a58100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphics_scatter(results_fairness,\n",
    "                     results_nofairness,\n",
    "                     comparisons,\n",
    "                     horizontal=True,\n",
    "                     save_figs=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    results_fairness:   dictionary of the structure\n",
    "                        {model alias: {diameter: {'ACCs': list of accuracies,\n",
    "                                                  'EOs':  list of equal opportunities,\n",
    "                                                  'DIs':  list of disparate impacts,\n",
    "                                                  'Cs':   list of hyperparameters\n",
    "                                                  }\n",
    "                                       }\n",
    "                         }\n",
    "    results_nofairness: dictionary of the structure\n",
    "                        {model alias: {diameter: {'acc': float of accuracy,\n",
    "                                                  'eo':  float of equal opportunity,\n",
    "                                                  'di':  float of disparate impact,\n",
    "                                                  'c':   float of hyperparameter\n",
    "                                                  }\n",
    "                                       }\n",
    "                         }\n",
    "    comparisons:         dictionary with key:fairness algorithm alias,\n",
    "                         value:comparison algorithm alias\n",
    "    horizontal:          boolean to determine whether to obtain a 3x1 or 1x3 plot\n",
    "    save_figs:           boolean to determine whether creates grahps should be saved\n",
    "    \"\"\"\n",
    "    \n",
    "    # choose matplotlib color palette\n",
    "    nb_colors = len(list(results_fairness[list(results_fairness.keys())[0]].keys()))\n",
    "    color_map = cm.get_cmap(name='Blues', lut=1000)\n",
    "    sub_color_map = color_map(np.linspace(0.9, 0.2, nb_colors))\n",
    "            \n",
    "    # --- plot scatter plots for coherence of accuracy and equal opportunity  \n",
    "    if horizontal:\n",
    "        plt.figure(figsize=(18,5)) \n",
    "    else:\n",
    "        plt.figure(figsize=(5,12))\n",
    "        \n",
    "    for i,diameter in enumerate(results_fairness.keys()):\n",
    "        \n",
    "        if horizontal:\n",
    "            plt.subplot(1,3,i+1)\n",
    "        else:\n",
    "            plt.subplot(3,1,i+1)\n",
    "        for j,(color,alias) in enumerate(zip(sub_color_map,results_fairness[diameter].keys())):\n",
    "            \n",
    "            # access fairness results\n",
    "            ACCs = results_fairness[diameter][alias]['ACCs'].copy()\n",
    "            EOs = results_fairness[diameter][alias]['EOs'].copy()\n",
    "            # shift the first entry, corresponding to the value (0,0.5)\n",
    "            # a little bit as else, the point would overlap perfectly\n",
    "            EOs[0] = EOs[0]+j*0.0025\n",
    "            \n",
    "            # access comparison (non-fairness) results\n",
    "            acc = results_nofairness[diameter][comparisons[alias]]['acc']\n",
    "            eo = results_nofairness[diameter][comparisons[alias]]['eo']\n",
    "            \n",
    "            # accuracy vs. equal opportunity\n",
    "            plt.scatter([eo],[acc], color=color, marker='x')\n",
    "            plt.scatter(EOs, ACCs, \n",
    "                        color=color,\n",
    "                        label='d={}, method={}'.format(diameter,\n",
    "                                                       alias))\n",
    "        if horizontal:\n",
    "            size1 = 'large'\n",
    "            size2 = 'x-large'\n",
    "            x_pos = 0.51\n",
    "            y_pos = 0.99\n",
    "        else:\n",
    "            size1 = 'medium'\n",
    "            size2 = 'large'\n",
    "            x_pos = 0.55\n",
    "            y_pos = 0.99\n",
    "            \n",
    "        plt.xlabel('equal opportunity',\n",
    "                   fontsize=size1)\n",
    "        plt.ylabel('accuracy',\n",
    "                   fontsize=size1)\n",
    "        plt.legend(fontsize=size1,\n",
    "                   loc='lower right')\n",
    "    \n",
    "    plt.suptitle('Coherence of accuracy and equal opportunity',\n",
    "                 fontsize=size2,\n",
    "                 x=x_pos, y=y_pos)\n",
    "    plt.tight_layout()\n",
    "    if save_figs:\n",
    "        plt.savefig('ACCandEOCoherence.pdf',\n",
    "                    format='pdf')\n",
    "    plt.show()\n",
    "    \n",
    "    # --- plot scatter plots for coherence of accuracy and disparate impact\n",
    "    if horizontal:\n",
    "        plt.figure(figsize=(18,5)) \n",
    "    else:\n",
    "        plt.figure(figsize=(5,12))\n",
    "        \n",
    "    for i,diameter in enumerate(results_fairness.keys()):\n",
    "        \n",
    "        if horizontal:\n",
    "            plt.subplot(1,3,i+1)\n",
    "        else:\n",
    "            plt.subplot(3,1,i+1)\n",
    "        for j,(color,alias) in enumerate(zip(sub_color_map,results_fairness[diameter].keys())):\n",
    "            \n",
    "            # access fairness results\n",
    "            ACCs = results_fairness[diameter][alias]['ACCs'].copy()\n",
    "            DIs = results_fairness[diameter][alias]['DIs'].copy()\n",
    "            # shift the first entry, corresponding to the value (1,0.5)\n",
    "            # a little bit as else, the point would overlap perfectly\n",
    "            DIs[0] = DIs[0]-j*0.0025\n",
    "            \n",
    "            # access comparison (non-fairness) results\n",
    "            acc = results_nofairness[diameter][comparisons[alias]]['acc']\n",
    "            di = results_nofairness[diameter][comparisons[alias]]['di']\n",
    "            \n",
    "            # accuracy vs. disparate impact\n",
    "            plt.scatter([di],[acc], color=color, marker='x')\n",
    "            plt.scatter(DIs, ACCs, \n",
    "                        color=color,\n",
    "                        label='d={}, method={}'.format(diameter,\n",
    "                                                       alias))\n",
    "        if horizontal:\n",
    "            size1 = 'large'\n",
    "            size2 = 'x-large'\n",
    "            x_pos = 0.52\n",
    "            y_pos = 0.99\n",
    "        else:\n",
    "            size1 = 'medium'\n",
    "            size2 = 'large'\n",
    "            x_pos = 0.56\n",
    "            y_pos = 0.99\n",
    "        \n",
    "        plt.xlabel('disparate impact',\n",
    "                   fontsize=size1) \n",
    "        plt.ylabel('accuracy', \n",
    "                   fontsize=size1)\n",
    "        plt.legend(fontsize=size1, \n",
    "                   loc='lower left')\n",
    "    \n",
    "    plt.suptitle('Coherence of accuracy and disparate impact',\n",
    "                 fontsize=size2, \n",
    "                 x=x_pos, y=y_pos)\n",
    "    plt.tight_layout()\n",
    "    if save_figs:\n",
    "        plt.savefig('ACCandDICoherence.pdf',\n",
    "                    format='pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242d2b19-952e-4fc9-8fee-56f6ff786b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graphics_lines(results_fairness,\n",
    "                   results_nofairness,\n",
    "                   comparisons,\n",
    "                   with_eo=False,\n",
    "                   save_figs=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    results_fairness:   dictionary of the structure\n",
    "                        {model alias: {diameter: {'ACCs': list of accuracies,\n",
    "                                                  'EOs':  list of equal opportunities,\n",
    "                                                  'DIs':  list of disparate impacts,\n",
    "                                                  'Cs':   list of hyperparameters\n",
    "                                                  }\n",
    "                                       }\n",
    "                        }\n",
    "    results_nofairness: dictionary of the structure\n",
    "                        {model alias: {diameter: {'acc': float of accuracy,\n",
    "                                                  'eo':  float of equal opportunity,\n",
    "                                                  'di':  float of disparate impact,\n",
    "                                                  'c':   float of hyperparameter\n",
    "                                                  }\n",
    "                                       }\n",
    "                         }\n",
    "    comparisons:         dictionary with key:fairness algorithm alias,\n",
    "                         value:comparison algorithm alias\n",
    "    with_eo:             boolean to determine whether \n",
    "                         the equal opportunity should be plotted\n",
    "    save_figs:           boolean to determine whether \n",
    "                         created grahps should be saved\n",
    "    \"\"\"\n",
    "\n",
    "    # ----- OPTION 1: Rows: Diameter, Columns: Method, In-Plots: Score\n",
    "    print('''OPTION 1''')\n",
    "    \n",
    "    # --- plot line plots for coherence of \n",
    "    # --- accuracy, equal opportunity and disparate impact, resp.,\n",
    "    # --- and the hyperparameter\n",
    "    \n",
    "    # choose matplotlib color palette\n",
    "    color_map = cm.get_cmap(name='Blues', lut=1000)\n",
    "    sub_color_map = color_map(np.linspace(0.9, 0.2, 3))\n",
    "    \n",
    "    if with_eo:\n",
    "        plt.figure(figsize=(20,25))\n",
    "    else:    \n",
    "        plt.figure(figsize=(20,15))\n",
    "    \n",
    "    i = 1\n",
    "    nb_methods = len(list(results_fairness[list(results_fairness.keys())[0]].keys()))\n",
    "    for diameter in results_fairness.keys():\n",
    "        \n",
    "        for alias in results_fairness[diameter].keys():\n",
    "    \n",
    "            # access fairness results\n",
    "            ACCs = results_fairness[diameter][alias]['ACCs']\n",
    "            EOs = results_fairness[diameter][alias]['EOs']\n",
    "            DIs = results_fairness[diameter][alias]['DIs']\n",
    "            if 'Cs' in list(results_fairness[diameter][alias].keys()):\n",
    "                Cs = results_fairness[diameter][alias]['Cs']\n",
    "                hyperparameter = 'covariance hyperparameter'\n",
    "            if 'Lambdas' in list(results_fairness[diameter][alias].keys()):\n",
    "                Cs = results_fairness[diameter][alias]['Lambdas']\n",
    "                hyperparameter = 'lambda hyperparameter'\n",
    "    \n",
    "            plt.subplot(3,nb_methods,i)\n",
    "            \n",
    "            # accuracy vs. hyperparameter\n",
    "            plt.plot(Cs, ACCs, \n",
    "                     color=sub_color_map[0],\n",
    "                     label='accuracy')\n",
    "            plt.scatter(Cs, ACCs, \n",
    "                        color=sub_color_map[0],\n",
    "                        marker='x')\n",
    "            \n",
    "            # equal opportunity vs. hyperparameter\n",
    "            if with_eo:\n",
    "                plt.plot(Cs, EOs, \n",
    "                         color=sub_color_map[2],\n",
    "                         label='equal opportunity')\n",
    "                plt.scatter(Cs, EOs, \n",
    "                             color=sub_color_map[2],\n",
    "                             marker='x')\n",
    "            \n",
    "            # disparate impact vs. hyperparameter\n",
    "            plt.plot(Cs, DIs, \n",
    "                    color=sub_color_map[1],\n",
    "                     label='disparate impact')\n",
    "            plt.scatter(Cs, DIs, \n",
    "                        color=sub_color_map[1],\n",
    "                        marker='x')\n",
    "            \n",
    "            plt.xticks(rotation=45)\n",
    "            plt.xlabel(hyperparameter,\n",
    "                       fontsize='large')\n",
    "            if with_eo:\n",
    "                plt.ylabel('accuracy, equal opportunity and disparate impact',\n",
    "                           fontsize='large')\n",
    "            else:\n",
    "                plt.ylabel('accuracy and disparate impact',\n",
    "                           fontsize='large')\n",
    "            plt.legend(fontsize='large')\n",
    "            plt.title('d={}, method={}'.format(diameter,\n",
    "                                               alias),\n",
    "                      fontsize='large')\n",
    "            i += 1\n",
    "    \n",
    "    if with_eo:\n",
    "        plt.suptitle('Accuracy, equal opportunity and disparate impact\\n'\\\n",
    "                     'in dependence on the method and its hyperparameter',\n",
    "                     fontsize='x-large',\n",
    "                     x=0.52, y=0.99)\n",
    "        plt.tight_layout()\n",
    "        if save_figs:\n",
    "            plt.savefig('ACCandEOandDIvsHyperparameterCoherence.pdf',\n",
    "                        format='pdf')\n",
    "    else:\n",
    "        plt.suptitle('Accuracy and disparate impact\\n'\\\n",
    "                     'in dependence on the method and its hyperparameter', \n",
    "                     fontsize='x-large',\n",
    "                     x=0.52, y=0.99)\n",
    "        plt.tight_layout()\n",
    "        if save_figs:\n",
    "            plt.savefig('ACCandDIvsHyperparameterCoherence.pdf',\n",
    "                        format='pdf')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    # ----- OPTION 2: Rows: Diameter, Method, Columns: Score\n",
    "    print('''OPTION 2''')\n",
    "    \n",
    "    # --- plot line plots for coherence of \n",
    "    # --- accuracy, equal opportunity and disparate impact, resp.,\n",
    "    # --- and the hyperparameter\n",
    "    \n",
    "    # choose matplotlib color palette\n",
    "    color_map = cm.get_cmap(name='Blues', lut=1000)\n",
    "    sub_color_map = color_map(np.linspace(1.0, 0.2, 3))\n",
    "    \n",
    "    for diameter in results_fairness.keys():\n",
    "\n",
    "        for color,alias in zip(sub_color_map,results_fairness[diameter].keys()):\n",
    "        \n",
    "            # access fairness results\n",
    "            ACCs = results_fairness[diameter][alias]['ACCs']\n",
    "            EOs = results_fairness[diameter][alias]['EOs']\n",
    "            DIs = results_fairness[diameter][alias]['DIs']\n",
    "            if 'Cs' in list(results_fairness[diameter][alias].keys()):\n",
    "                Cs = results_fairness[diameter][alias]['Cs']\n",
    "                hyperparameter = 'covariance hyperparameter'\n",
    "            if 'Lambdas' in list(results_fairness[diameter][alias].keys()):\n",
    "                Cs = results_fairness[diameter][alias]['Lambdas']\n",
    "                hyperparameter = 'lambda hyperparameter'\n",
    "                \n",
    "            plt.figure(figsize=(20,3))\n",
    "    \n",
    "            # accuracy vs. hyperparameter\n",
    "            plt.subplot(1,3,1)\n",
    "            plt.plot(Cs, ACCs, \n",
    "                     color=color,\n",
    "                     label='d={}, method={}'.format(diameter,\n",
    "                                                       alias))\n",
    "            plt.scatter(Cs, ACCs, \n",
    "                        color=color,\n",
    "                        marker='x')\n",
    "            plt.xlabel(hyperparameter)\n",
    "            plt.ylabel('accuracy')\n",
    "            \n",
    "            # equal opportunity vs. hyperparameter\n",
    "            plt.subplot(1,3,2)\n",
    "            plt.plot(Cs, EOs, \n",
    "                     color=color,\n",
    "                     label='d={}, method={}'.format(diameter,\n",
    "                                                    alias))\n",
    "            plt.scatter(Cs, EOs, \n",
    "                         color=color,\n",
    "                         marker='x')\n",
    "            plt.xlabel(hyperparameter)\n",
    "            plt.ylabel('equal opportunity')\n",
    "            \n",
    "             # disparate impact vs. hyperparameter\n",
    "            plt.subplot(1,3,3)\n",
    "            plt.plot(Cs, DIs, \n",
    "                     color=color,\n",
    "                     label='d={}, method={}'.format(diameter,\n",
    "                                                    alias))\n",
    "            plt.scatter(Cs, DIs, \n",
    "                        color=color,\n",
    "                        marker='x')\n",
    "            plt.xlabel(hyperparameter)\n",
    "            plt.ylabel('disparate impact')\n",
    "            plt.legend(loc='upper left', bbox_to_anchor=(1, 1.01))\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b062ee7-59f0-429d-8fcc-35bc6b285b5a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Pipeline application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8399ae5e-9cde-4a4c-89e4-137c887e353a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216f67b3-b0d2-4e09-b4c5-dc22d2c9d7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define time ID from which starting we want to use the data\n",
    "# (data before is if used, only used for preprocessing)\n",
    "time_start = 100\n",
    "# define time window used for the rolling mean\n",
    "time_wind = 3\n",
    "# define the classifiers used per sensor node\n",
    "classifier = ThresholdClassification\n",
    "classifier_approx = ThresholdClassificationApproximation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b5b703-6673-458b-969d-a040e7b5a6b2",
   "metadata": {},
   "source": [
    "We test different combinations of sensors actually used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e505392c-36f5-44b4-ad26-0cfed1e6993b",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_ids = list(df_information.loc[:,'node ID'])\n",
    "sensor_ids = list(df_leaks.columns[[0,1,3]])\n",
    "print('Given sensors: {}'.format(sensor_ids))\n",
    "\n",
    "sensitive_features = list(df_leaks.columns[8:11])\n",
    "print('Given sensitive features: {}'.format(sensitive_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25280567-94ae-43ff-91bc-0013a8b2427f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Visualization - Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b5f6e1-1cc4-487f-aa97-5dce7589a3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wn_hanoi = wntr.network.WaterNetworkModel('../1_FeatureGeneration/models/Hanoi.inp') \n",
    "\n",
    "plot_network(node_ids=node_ids,\n",
    "             sensor_ids=sensor_ids,\n",
    "             df_information=df_information,\n",
    "             wn=wn_hanoi,\n",
    "             name='Hanoi',\n",
    "             save_figs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbc4fea-0d6f-4fb8-bfae-ec0144059fe9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Preprocessing for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf12e6c6-fca0-4edf-ba8b-f7b6b979a118",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "preprocessor = Preprocessing_RollingMean(time_start=time_start,\n",
    "                                         time_wind=time_wind)\n",
    "X_pre = preprocessor.transform(df_leaks.loc[:,sensor_ids])\n",
    "Y_pre = df_leaks.loc[time_start:,sensor_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09319e18-c615-40d1-a593-9336ea353ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22de6ab7-d0f1-4ed0-9db6-9d7b8755dc26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Y_pre"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffb23c1-f7a4-46e1-beba-d6a5ec41dc22",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Visualization - True Pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203fe8b3-8f45-4df0-a948-ec621cc7f3dc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_data_per_timeindex(df_leaks, # same as using Y_pred\n",
    "                        sensor_ids=sensor_ids,\n",
    "                        start_ids=[128000],\n",
    "                        end_ids=[129000],\n",
    "                        #thresholds={'3':0.5, '10':0.2, '23':1, '25':0.5},\n",
    "                        show_legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02fbe28-edbe-4aa8-89c6-e55bbbba3f43",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_data_per_timeindex_and_sensor(dfs={'pressure with no leak':df_noleaks,\n",
    "                                        'pressure with potentially leak':df_leaks}, # same as using Y_pred\n",
    "                                        #'predicted pressure':..},\n",
    "                                   sensor_ids=sensor_ids,\n",
    "                                   start_ids=[128000],\n",
    "                                   end_ids=[129000],\n",
    "                                   #thresholds={'3':0.5, '10':2, '23':0.2, '25':0.5},\n",
    "                                   #threshold_key='pressure with potentially leak',\n",
    "                                   show_legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd27b8d2-2b00-4c04-be84-c1f8b6f93951",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_data_per_setting(df_leaks, # same as using Y_pred\n",
    "                      df_information=df_information,\n",
    "                      sensor_ids=sensor_ids,\n",
    "                      node_ids=['2','5','16'],\n",
    "                      diameters=[10],\n",
    "                      setting_ids=None,\n",
    "                      #thresholds={'3':0.5, '10':2, '23':0.2, '25':0.5},\n",
    "                      time_puffer=100,\n",
    "                      show_legend=True,\n",
    "                      zoom_leak=True,\n",
    "                      print_report=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba1f384-003d-487c-b7af-6cbfe7734761",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_data_per_setting_and_sensor(dfs={'pressure with no leak':df_noleaks,\n",
    "                                      'pressure with leak':df_leaks}, # same as using Y_pred\n",
    "                                      #'predicted pressure':...},\n",
    "                                 df_information=df_information,\n",
    "                                 sensor_ids=sensor_ids,\n",
    "                                 node_ids=['2','5','16'],\n",
    "                                 diameters=[10],\n",
    "                                 setting_ids=None,\n",
    "                                 thresholds={'3':0.5, '10':2, '23':0.2, '25':0.5},\n",
    "                                 #threshold_key='pressure with leak',\n",
    "                                 leak_key='pressure with leak',\n",
    "                                 time_puffer=100,\n",
    "                                 show_legend=True,\n",
    "                                 zoom_leak=True,\n",
    "                                 print_report=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e96d3c-143b-4fa7-8bc3-bc9985e65a67",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Regression - Virtual Sensors\n",
    "\n",
    "In df_information we find that the first leak appears at time ID 1541. Therefore, we use the preprocessed data up to the time ID 1540 to train the virtual sensors on non-leaky data. We use KFold-cross validation to evaluate the virtual sensors, where as a score, we used the mean r2 score and the mean RMSE over all folds *and* all sensors (as we receive a score per sensor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc627e06-13fc-478c-ab73-d907755f1415",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "time_end_noleaks = df_information.loc[1,'setting start ID'] - 1\n",
    "print('Time ID before first leak starts:', time_end_noleaks)\n",
    "X_reg_train = X_pre.loc[time_start:time_end_noleaks,:]\n",
    "Y_reg_train = Y_pre.loc[time_start:time_end_noleaks,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b43c5b-a80a-41fe-b904-5c06b06c9e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_reg_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405994a9-3a58-4752-9e04-1b35ee16a79f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Y_reg_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff9b03c-9680-412b-a610-ef88da0cb4a7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ----- evaluation by cross validation\n",
    "\n",
    "# load dataset\n",
    "X = X_reg_train\n",
    "Y = Y_reg_train\n",
    "\n",
    "# --- perform evaluation by cross validation\n",
    "train_r2s = list()\n",
    "test_r2s = list()\n",
    "train_rmses = list()\n",
    "test_rmses = list()\n",
    "# instantiate cross validation object\n",
    "cv = KFold(n_splits=8, shuffle=False)\n",
    "for train_index, test_index in cv.split(X):\n",
    "    # compute cross validation folds (shift as data indices begin at time_start)\n",
    "    X_train = X.loc[train_index+time_start,:]\n",
    "    Y_train = Y.loc[train_index+time_start,:]\n",
    "    X_test = X.loc[test_index+time_start,:]\n",
    "    Y_test = Y.loc[test_index+time_start,:]\n",
    "    \n",
    "    # --- perform evaluation by training and testing\n",
    "    # instantiate model\n",
    "    regressor = LinearRegression\n",
    "    model = MultiRegression(regressor)\n",
    "    # train model\n",
    "    model.fit(X_train, Y_train)\n",
    "    # test model\n",
    "    train_r2s.append(model.score(X_train, Y_train)[0])\n",
    "    test_r2s.append(model.score(X_test, Y_test)[0])\n",
    "    train_rmses.append(model.score(X_train, Y_train)[1])\n",
    "    test_rmses.append(model.score(X_test, Y_test)[1])\n",
    "\n",
    "# --- access results\n",
    "print('Mean and variance of training r2 scores over all folds: '\\\n",
    "      '{0:0.5f} pm {1:0.5f}'.format(np.array(train_r2s).mean(),\n",
    "                                    np.array(train_r2s).var())) \n",
    "print('Mean and variance of test r2 scores over all folds: '\\\n",
    "      '{0:0.5f} pm {1:0.5f}'.format(np.array(test_r2s).mean(),\n",
    "                                    np.array(test_r2s).var())) \n",
    "print('Train scores:\\n', train_r2s)\n",
    "print('Test scores:\\n', test_r2s)\n",
    "print('Mean and variance of training rmse over all folds: '\\\n",
    "      '{0:0.5f} pm {1:0.5f}'.format(np.array(train_rmses).mean(),\n",
    "                                    np.array(train_rmses).var())) \n",
    "print('Mean and variance of test r2 scores over all folds: '\\\n",
    "      '{0:0.5f} pm {1:0.5f}'.format(np.array(test_rmses).mean(),\n",
    "                                    np.array(test_rmses).var())) \n",
    "print('Train scores:\\n', train_rmses)\n",
    "print('Test scores:\\n', test_rmses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2834ea62-95c3-4477-9939-3b2e0b4d369e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# refit regression model on whole non-leaky data\n",
    "regressor = LinearRegression\n",
    "model_reg = MultiRegression(regressor)\n",
    "model_reg.fit(X_reg_train, Y_reg_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab802c8-b968-43e7-af07-63c3a917e284",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Preprocessing for Classification - Compute Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5450ce2-58ad-424d-9251-f688796f59a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start_leaks = df_information.loc[1,'setting start ID']\n",
    "print('Time ID where first leak starts:', time_start_leaks)\n",
    "X_reg_test = X_pre.loc[time_start_leaks:,:]\n",
    "Y_reg_test = Y_pre.loc[time_start_leaks:,:]\n",
    "\n",
    "# apply regression model on whole (not yet seen) data\n",
    "Y_reg_pred =  model_reg.predict(X_reg_test)\n",
    "\n",
    "# compute residuals based on true data and predicted data\n",
    "X_clas = (Y_reg_test - Y_reg_pred).abs()\n",
    "\n",
    "X_sen = df_leaks.loc[time_start_leaks:,sensitive_features]\n",
    "y_clas = df_leaks.loc[time_start_leaks:,['y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca588806-a1b7-4fcf-8a5e-af4ebb6af722",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_reg_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2693ac2f-7a7f-4dc3-a30b-5ebd95f47551",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y_reg_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878b0f05-affc-41b4-a067-ccce970706e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y_reg_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cdb0f2-bbd5-483c-b569-fd72c64f97a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_clas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5203e612-2c23-4da7-9989-38e8e1b98ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8926e032-71f6-47b7-82fb-1f7e9a79ff06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_clas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef2c81d-75dd-4b41-9fcd-d5c082a505c5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Visualization - True and Predicted Pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1990cc-1255-42a6-bd24-862f2573dcf1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_data_per_timeindex_and_sensor(dfs={'pressure with no leak':df_noleaks,\n",
    "                                        'pressure with potentially leak':Y_reg_test, # part of df_leaks\n",
    "                                        'predicted pressure':Y_reg_pred},\n",
    "                                   sensor_ids=sensor_ids,\n",
    "                                   start_ids=[128000],\n",
    "                                   end_ids=[129000],\n",
    "                                   #thresholds={'3':0.5, '10':2, '23':0.2, '25':0.5},\n",
    "                                   #threshold_key='pressure with potentially leak',\n",
    "                                   show_legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82529a07-91e2-46ed-8aca-a3c4a450a7b7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_data_per_setting_and_sensor(dfs={#'pressure with no leak':df_noleaks,\n",
    "                                      'pressure with leak':Y_reg_test, # part of df_leaks\n",
    "                                      'predicted pressure':Y_reg_pred},\n",
    "                                 df_information=df_information,\n",
    "                                 sensor_ids=sensor_ids,\n",
    "                                 node_ids=[str(x) for x in range(2,33)],\n",
    "                                 diameters=[10],\n",
    "                                 setting_ids=None,\n",
    "                                 thresholds={'3':0.5, '10':2, '23':0.2, '25':0.5},\n",
    "                                 #threshold_key='pressure with leak',\n",
    "                                 leak_key='pressure with leak',\n",
    "                                 time_puffer=100,\n",
    "                                 show_legend=True,\n",
    "                                 zoom_leak=True,\n",
    "                                 print_report=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a5e2aa-4a06-4029-b0bb-7677db54b558",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot pressure *residuals*\n",
    "plot_data_per_setting(X_clas,\n",
    "                      df_information=df_information,\n",
    "                      sensor_ids=sensor_ids,\n",
    "                      node_ids=[str(x) for x in range(2,33)],\n",
    "                      diameters=[10],\n",
    "                      setting_ids=None,\n",
    "                      #thresholds={'3':0.5, '10':2, '23':0.2, '25':0.5},\n",
    "                      time_puffer=100,\n",
    "                      show_legend=True,\n",
    "                      zoom_leak=True,\n",
    "                      print_report=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea139ae-a4b1-4e6a-ba65-02080de8792e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Classification - Leak Detector(s)\n",
    "\n",
    "We now use the virtual sensors to predict the pressure even for times where a leak is active in the WDN. We make use of the residuals $|p_j(t_i) - f_j^r(p_{\\neq j}(t_i))| \\in \\mathbb{R}$ to define a threshold-based classifier that predicts whether a leak is active (1) or not (0) at time $t_i$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d368cbb-5f95-4c23-9eed-1dc882c48749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary to store all results\n",
    "# which are visualized at the end\n",
    "results_fairness= dict()\n",
    "results_nofairness = dict()\n",
    "# define which fairness method\n",
    "# should improve which non-fairness method\n",
    "comparisons = {'T-F-PR+F':'T-F-PR',\n",
    "               'ACC+F':'ACC',\n",
    "               'DI+ACC':'ACC'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94f461c-6a98-46e1-8f28-2332678fc279",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Diameter = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281d501f-516a-4df6-8e14-c9a3aff01a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the training and test data according to the diameter\n",
    "diameter = 5\n",
    "results = filter_diameter(X_clas, y_clas, X_sen, diameter=diameter, df_information=df_information)\n",
    "X_clas_train, X_clas_test, y_clas_train, y_clas_test, X_sen_train, X_sen_test = results\n",
    "print(X_sen_train.sum())\n",
    "print(X_sen_test.sum())\n",
    "\n",
    "# create dictionary to store all results \n",
    "# which are visualized at the end\n",
    "results_d5 = dict()\n",
    "results_fairness[diameter] = dict()\n",
    "results_nofairness[diameter] = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7f0881-a9d5-4f2c-a843-01c82cc1f2d5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Method: Choose hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcc6fb6-a568-4dc2-9640-8e7fbae1dc1c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_clas = ETC_hyperparameter()\n",
    "model_clas.fit(X_clas_train, factor=0.15, print_coeff=True)\n",
    "acc,eo,di,TPRs = evaluate(model_clas)\n",
    "# define the starting point for all other algorithms\n",
    "start_thresholds = model_clas.thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784d90ae-ae71-4193-aa87-d7d685d2b989",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_TPR = max(list(TPRs.values()))\n",
    "min_TPR = min(list(TPRs.values()))\n",
    "print('{} & {} & {} & {} & {} & {} & {}'.format(round(acc, 4),\n",
    "                                                round(max_TPR, 4),\n",
    "                                                round(min_TPR, 4),\n",
    "                                                round(di, 4),\n",
    "                                                round(eo, 4),\n",
    "                                                round((1-eo/max_TPR), 4),\n",
    "                                                round((1-di)*max_TPR, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d5954a-135d-4a95-aeab-d3071c66cfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_d5[model_clas] = {'acc':acc,'eo':eo,'di':di,'TPRs':TPRs}\n",
    "results_nofairness[diameter][model_clas.alias] = dict()\n",
    "results_nofairness[diameter][model_clas.alias]['acc'] = acc\n",
    "results_nofairness[diameter][model_clas.alias]['eo'] = eo\n",
    "results_nofairness[diameter][model_clas.alias]['di']= di"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251a47e9-3a85-4935-b304-1180d86b0573",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Method: Optimize TPR - FPR  (db.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0ecedb-b6b5-43a7-a0df-6cbc7f0720b9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "b_sigmoid=100\n",
    "sum_threshold=0.8\n",
    "model_clas = ETC_optimizeFTPR_db()\n",
    "model_clas.fit(X_clas_train, \n",
    "               y_clas_train,\n",
    "               start_thresholds=start_thresholds,\n",
    "               b_sigmoid=b_sigmoid, \n",
    "               sum_threshold=sum_threshold,\n",
    "               print_coeff=True)\n",
    "acc,eo,di,TPRs = evaluate(model_clas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ab7834-7b77-4ddc-a8be-a3e8b52ce07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_d5[model_clas] = {'acc':acc,'eo':eo,'di':di,'TPRs':TPRs}\n",
    "results_nofairness[diameter][model_clas.alias] = dict()\n",
    "results_nofairness[diameter][model_clas.alias]['acc'] = acc\n",
    "results_nofairness[diameter][model_clas.alias]['eo'] = eo\n",
    "results_nofairness[diameter][model_clas.alias]['di']= di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef87252-f494-4314-b76a-77d06ef2d630",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_, y_pred_approx = model_clas.predict_approx(X_clas_train,\n",
    "                                             b_sigmoid=b_sigmoid, \n",
    "                                             sum_threshold=sum_threshold)\n",
    "\n",
    "sensitive_features = list(X_sen_train.columns)\n",
    "for sensitive_feature in sensitive_features:\n",
    "    x_sen = X_sen_train.loc[:,sensitive_feature]\n",
    "    print('Sensitive feature:', sensitive_feature)\n",
    "    print(Cov(x_sen, y_pred_approx.loc[:,'y']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25de17a6-8612-4492-8149-a32e276e6433",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Method: Optimize TPR - FPR while enhancing fairness (db.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e05a78-1f87-4abc-b5f1-be3bddf84e63",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "b_sigmoid=100\n",
    "sum_threshold=0.8\n",
    "model_clas = ETC_optimizeFTPR_F_db()\n",
    "model_clas.fit(X_clas_train,\n",
    "               X_sen_train,\n",
    "               y_clas_train,\n",
    "               start_thresholds=start_thresholds,\n",
    "               b_sigmoid=b_sigmoid, \n",
    "               sum_threshold=sum_threshold,\n",
    "               c=0.05, #0.045, 0.05!, 0.06, 0.07, 0.08 (not so nice series)\n",
    "               mu=0.1, #0.1\n",
    "               print_coeff=True)\n",
    "acc,eo,di,TPRs = evaluate(model_clas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c04e809-3f6d-40f5-b9e0-ad543c16114e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- specify hyperparameters and model class\n",
    "start_hyper = 0.05\n",
    "extra_hypers = [0.045]\n",
    "b_sigmoid = 100\n",
    "sum_threshold = 0.8\n",
    "model_clas = ETC_optimizeFTPR_F_db()\n",
    "\n",
    "# --- define constant for stopping criterium\n",
    "comparison_algo = comparisons[model_clas.alias]\n",
    "di_nofairness = results_nofairness[diameter][comparison_algo]['di']\n",
    "print('Disparate impact of approx. {} '\\\n",
    "      'from {} algorithm is used as a stopping criterium.'.format(round(di_nofairness,5),\n",
    "                                                                        comparison_algo))\n",
    "# ----- test model for different hyperparameters\n",
    "Cs = extra_hypers + [round(start_hyper+i*0.01,2) for i in range(0,40)]\n",
    "print('\\nHyperparameters to test:\\n', Cs)\n",
    "results_fairness[diameter][model_clas.alias] = dict()\n",
    "results_fairness[diameter][model_clas.alias]['ACCs'] = list()\n",
    "results_fairness[diameter][model_clas.alias]['EOs'] = list()\n",
    "results_fairness[diameter][model_clas.alias]['DIs'] = list()\n",
    "results_fairness[diameter][model_clas.alias]['Cs'] = list()\n",
    "for c in Cs:\n",
    "    print('\\nc:', c)\n",
    "    # --- train model for fixed hyperparameters\n",
    "    model_clas = ETC_optimizeFTPR_F_db()\n",
    "    model_clas.fit(X_clas_train,\n",
    "                   X_sen_train,\n",
    "                   y_clas_train,\n",
    "                   start_thresholds=start_thresholds,\n",
    "                   b_sigmoid=b_sigmoid, \n",
    "                   sum_threshold=sum_threshold,\n",
    "                   c=c, #0.045, 0.05!, 0.06, 0.07, 0.08\n",
    "                   mu=0.1, #0.1\n",
    "                   print_coeff=False)\n",
    "    # --- evaluate model for fixed hyperparameters\n",
    "    acc,eo,di,TPRs = evaluate(model_clas)\n",
    "    # --- store evaluation until model is as unfair as comparison model\n",
    "    if di <= di_nofairness:\n",
    "        print('\\nHyperparameter {} and larger were not used'.format(c))\n",
    "        break\n",
    "    results_d5[model_clas] = {'acc':acc,'eo':eo,'di':di,'TPRs':TPRs}\n",
    "    results_fairness[diameter][model_clas.alias]['ACCs'].append(acc)\n",
    "    results_fairness[diameter][model_clas.alias]['EOs'].append(eo)\n",
    "    results_fairness[diameter][model_clas.alias]['DIs'].append(di)\n",
    "    results_fairness[diameter][model_clas.alias]['Cs'].append(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056657ea-ea5b-41db-82e6-a7d97fffe816",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Method: Optimize TPR - FPR  (ndb.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca06a745-a9f4-4d4c-8021-305b3da2108b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model_clas = ETC_optimizeFTPR_ndb()\n",
    "#model_clas.fit(X_clas_train, \n",
    "#               y_clas_train,\n",
    "#               start_thresholds=start_thresholds,\n",
    "#               print_coeff=True)\n",
    "#acc,eo,di,TPRs = evaluate(model_clas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27446b86-a024-4f02-9d0c-3d38e591f257",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Method: Optimize TPR - FPR while enhancing fairness (ndb.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72afedae-e0a8-4b88-8833-31229533e311",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model_clas = ETC_optimizeFTPR_F_ndb()\n",
    "#model_clas.fit(X_clas_train, \n",
    "#               X_sen_train,\n",
    "#               y_clas_train,\n",
    "#               start_thresholds=start_thresholds,\n",
    "#               c=0.05, #0.044, 0.05!, 0.06, 0.07\n",
    "#               mu=0.2, #0.2\n",
    "#               print_coeff=True)\n",
    "#acc,eo,di,TPRs = evaluate(model_clas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3878059b-10ad-4f52-9b29-f37b9e3aabe5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Method: Optimize ACC (db.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b3dbf6-edf6-4db4-8a1e-a3ca81f8f4b6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "b_sigmoid=100\n",
    "sum_threshold=0.8\n",
    "model_clas = ETC_optimizeACC_db()\n",
    "model_clas.fit(X_clas_train,\n",
    "               y_clas_train,\n",
    "               start_thresholds=start_thresholds,\n",
    "               b_sigmoid=b_sigmoid, \n",
    "               sum_threshold=sum_threshold,\n",
    "               print_coeff=True)\n",
    "acc,eo,di,TPRs = evaluate(model_clas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0e6bf3-9955-4baa-8021-63067a08a9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_d5[model_clas] = {'acc':acc,'eo':eo,'di':di,'TPRs':TPRs}\n",
    "results_nofairness[diameter][model_clas.alias] = dict()\n",
    "results_nofairness[diameter][model_clas.alias]['acc'] = acc\n",
    "results_nofairness[diameter][model_clas.alias]['eo'] = eo\n",
    "results_nofairness[diameter][model_clas.alias]['di']= di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37a6f1b-bc1a-428f-b447-15182be3cc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_,_,_,_,acc_best,_ = model_clas.score(X_clas_train, \n",
    "                                        y_clas_train,\n",
    "                                        print_all_scores=False)\n",
    "acc_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4171228c-445e-457d-8ffa-525d4e49988e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_, y_pred_approx = model_clas.predict_approx(X_clas_train,\n",
    "                                             b_sigmoid=b_sigmoid, \n",
    "                                             sum_threshold=sum_threshold)\n",
    "\n",
    "sensitive_features = list(X_sen_train.columns)\n",
    "for sensitive_feature in sensitive_features:\n",
    "    x_sen = X_sen_train.loc[:,sensitive_feature]\n",
    "    print('Sensitive feature:', sensitive_feature)\n",
    "    print(Cov(x_sen, y_pred_approx.loc[:,'y']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64860e63-74ae-4724-a1f7-92a405dd3007",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Method: Optimize ACC while enhancing fairness (db.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab376abc-d740-44cc-a1a4-2ed821577c5d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "b_sigmoid=100\n",
    "sum_threshold=0.8\n",
    "model_clas = ETC_optimizeACC_F_db()\n",
    "model_clas.fit(X_clas_train,\n",
    "               X_sen_train,\n",
    "               y_clas_train,\n",
    "               start_thresholds=start_thresholds,\n",
    "               b_sigmoid=b_sigmoid, \n",
    "               sum_threshold=sum_threshold,\n",
    "               c=0.07, #0.05, 0.055, 0.06, 0.07!, 0.08, ..., 0.17\n",
    "               mu=0.15, #0.15\n",
    "               print_coeff=True)\n",
    "acc,eo,di,TPRs = evaluate(model_clas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6074d61-3437-443c-bd26-cd56af3ebeac",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- specify hyperparameters and model class\n",
    "start_hyper = 0.06\n",
    "extra_hypers = [0.05, 0.055]\n",
    "b_sigmoid=100\n",
    "sum_threshold=0.8\n",
    "model_clas = ETC_optimizeACC_F_db()\n",
    "\n",
    "# --- define constant for stopping criterium\n",
    "comparison_algo = comparisons[model_clas.alias]\n",
    "di_nofairness = results_nofairness[diameter][comparison_algo]['di']\n",
    "print('disparate impact of approx. {} '\\\n",
    "      'from {} algorithm is used as a stopping criterium.'.format(round(di_nofairness,5),\n",
    "                                                                        comparison_algo))\n",
    "# ----- test model for different hyperparameters\n",
    "Cs = extra_hypers + [round(start_hyper+i*0.01,2) for i in range(0,40)]\n",
    "print('\\nHyperparameters to test:\\n', Cs)\n",
    "results_fairness[diameter][model_clas.alias] = dict()\n",
    "results_fairness[diameter][model_clas.alias]['ACCs'] = list()\n",
    "results_fairness[diameter][model_clas.alias]['EOs'] = list()\n",
    "results_fairness[diameter][model_clas.alias]['DIs'] = list()\n",
    "results_fairness[diameter][model_clas.alias]['Cs'] = list()\n",
    "for c in Cs:\n",
    "    print('\\nc:', c)\n",
    "    # --- train model for fixed hyperparameters\n",
    "    model_clas = ETC_optimizeACC_F_db()\n",
    "    model_clas.fit(X_clas_train,\n",
    "                   X_sen_train,\n",
    "                   y_clas_train,\n",
    "                   start_thresholds=start_thresholds,\n",
    "                   b_sigmoid=b_sigmoid, \n",
    "                   sum_threshold=sum_threshold,\n",
    "                   c=c, #0.05, 0.055, 0.06, 0.07!, 0.08, ..., 0.17\n",
    "                   mu=0.15, #0.15\n",
    "                   print_coeff=False)\n",
    "    # --- evaluate model for fixed hyperparameters\n",
    "    acc,eo,di,TPRs = evaluate(model_clas)\n",
    "    # --- store evaluation until model is as unfair as comparison model\n",
    "    if di <= di_nofairness:\n",
    "        print('\\nHyperparameter {} and larger were not used'.format(c))\n",
    "        break\n",
    "    results_d5[model_clas] = {'acc':acc,'eo':eo,'di':di,'TPRs':TPRs}\n",
    "    results_fairness[diameter][model_clas.alias]['ACCs'].append(acc)\n",
    "    results_fairness[diameter][model_clas.alias]['EOs'].append(eo)\n",
    "    results_fairness[diameter][model_clas.alias]['DIs'].append(di)\n",
    "    results_fairness[diameter][model_clas.alias]['Cs'].append(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622384df-55ea-4bfe-b76c-09b3607796c9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Method: Optimize ACC (ndb.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce064a5c-5a6a-4d6a-81e3-7d188fdc10ab",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model_clas = ETC_optimizeACC_ndb()\n",
    "#model_clas.fit(X_clas_train, \n",
    "#               y_clas_train,\n",
    "#               start_thresholds=start_thresholds,\n",
    "#               print_coeff=True)\n",
    "#acc,eo,di,TPRs = evaluate(model_clas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86531df-941c-482a-ac55-9ccef520d9a4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Method: Optimize ACC while enhancing fairness (ndb.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94ae531-9780-4156-94de-6f6761858464",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model_clas = ETC_optimizeACC_F_ndb()\n",
    "#model_clas.fit(X_clas_train, \n",
    "#               X_sen_train,\n",
    "#               y_clas_train,\n",
    "#               start_thresholds=start_thresholds,\n",
    "#               c=0.07, #0.045, 0.05, 0.06, 0.07!\n",
    "#               mu=0.2, #0.2\n",
    "#               print_coeff=True)\n",
    "#acc,eo,di,TPRs = evaluate(model_clas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2227ace-6157-4a2f-8f56-20a835cdfe41",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Method: Optimize DI (ndb.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f8917e-2c61-43d1-9830-695a870143d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a2a3eb-f65f-4566-8c29-eff1c09638a1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_clas = ETC_optimizeDI_ndb()\n",
    "model_clas.fit(X_clas_train,\n",
    "               X_sen_train,\n",
    "               y_clas_train,\n",
    "               start_thresholds=start_thresholds,\n",
    "               mu=0.05, #0.05\n",
    "               lamb=0.04, #0.01, 0.02, 0.03, 0.04!, 0.05, ..., 0.2\n",
    "               acc_best=acc_best,\n",
    "               print_coeff=True)\n",
    "acc,eo,di,TPRs = evaluate(model_clas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6246b62d-ffe0-4e38-bf59-f41de14eece9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- specify hyperparameters and model class\n",
    "start_hyper = 0.2\n",
    "model_clas = ETC_optimizeDI_ndb()\n",
    "\n",
    "# --- define constant for stopping criterium\n",
    "comparison_algo = comparisons[model_clas.alias]\n",
    "di_nofairness = results_nofairness[diameter][comparison_algo]['di']\n",
    "print('disparate impact of approx. {} '\\\n",
    "      'from {} algorithm is used as a stopping criterium.'.format(round(di_nofairness,5),\n",
    "                                                                        comparison_algo))\n",
    "\n",
    "# --- test model for different hyperparameters\n",
    "Cs = [round(start_hyper-i*0.01,2) for i in range(0,40) if round(start_hyper-i*0.01,2)>0]\n",
    "print('\\nHyperparameters to test:\\n', Cs)\n",
    "results_fairness[diameter][model_clas.alias] = dict()\n",
    "results_fairness[diameter][model_clas.alias]['ACCs'] = list()\n",
    "results_fairness[diameter][model_clas.alias]['EOs'] = list()\n",
    "results_fairness[diameter][model_clas.alias]['DIs'] = list()\n",
    "results_fairness[diameter][model_clas.alias]['Lambdas'] = list()\n",
    "for lamb in Cs:\n",
    "    print('\\nlambda:', lamb)\n",
    "    # --- train model for fixed hyperparameters\n",
    "    model_clas = ETC_optimizeDI_ndb()\n",
    "    model_clas.fit(X_clas_train,\n",
    "                   X_sen_train,\n",
    "                   y_clas_train,\n",
    "                   start_thresholds=start_thresholds,\n",
    "                   mu=0.05, #0.05\n",
    "                   lamb=lamb, #0.01, 0.02, 0.03, 0.04!, 0.05, ..., 0.2\n",
    "                   acc_best=acc_best,\n",
    "                   print_coeff=False)\n",
    "    # --- evaluate model for fixed hyperparameters\n",
    "    acc,eo,di,TPRs = evaluate(model_clas)\n",
    "    # --- store evaluation until model is as unfair as comparison model\n",
    "    if di <= di_nofairness:\n",
    "        print('\\nHyperparameter {} and larger were not used'.format(lamb))\n",
    "        break\n",
    "    results_d5[model_clas] = {'acc':acc,'eo':eo,'di':di,'TPRs':TPRs}\n",
    "    results_fairness[diameter][model_clas.alias]['ACCs'].append(acc)\n",
    "    results_fairness[diameter][model_clas.alias]['EOs'].append(eo)\n",
    "    results_fairness[diameter][model_clas.alias]['DIs'].append(di)\n",
    "    results_fairness[diameter][model_clas.alias]['Lambdas'].append(lamb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772de22f-02ce-49c8-9880-9a07e3d40037",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Method: Optimize EO (ndb.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95af6a5d-479b-43b0-b83e-1c7b832897f3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model_clas = ETC_optimizeEO_ndb()\n",
    "#model_clas.fit(X_clas_train,\n",
    "#               X_sen_train,\n",
    "#               y_clas_train,\n",
    "#               start_thresholds=start_thresholds,\n",
    "#               mu=0.05, #0.05\n",
    "#               lamb=0.04, #0.01, 0.02, 0.03, 0.04!, 0.05, ..., 0.21\n",
    "#               acc_best=acc_best,\n",
    "#               print_coeff=True)\n",
    "#acc,eo,di,TPRs = evaluate(model_clas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0df703-ac2d-4a24-af48-aebfbf894baf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Diameter = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5886c5c8-fe58-4892-a275-fd475c173aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the training and test data according to the diameter\n",
    "diameter = 10\n",
    "results = filter_diameter(X_clas, y_clas, X_sen, diameter=diameter, df_information=df_information)\n",
    "X_clas_train, X_clas_test, y_clas_train, y_clas_test, X_sen_train, X_sen_test = results\n",
    "print(X_sen_train.sum())\n",
    "print(X_sen_test.sum())\n",
    "\n",
    "# create dictionary to store all results \n",
    "# which are visualized at the end\n",
    "results_d10 = dict()\n",
    "results_fairness[diameter] = dict()\n",
    "results_nofairness[diameter] = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c830f0-6003-4cd5-8079-8be700f2462e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Method: Choose hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd04bc2-0892-484a-a216-2518ed711d70",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_clas = ETC_hyperparameter()\n",
    "model_clas.fit(X_clas_train, factor=0.2, print_coeff=True)\n",
    "acc,eo,di,TPRs = evaluate(model_clas)\n",
    "# define the starting point for all other algorithms\n",
    "start_thresholds = model_clas.thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c070d658-94f3-42f6-9c6c-be323bdfcec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_TPR = max(list(TPRs.values()))\n",
    "min_TPR = min(list(TPRs.values()))\n",
    "print('{} & {} & {} & {} & {} & {} & {}'.format(round(acc, 4),\n",
    "                                                round(max_TPR, 4),\n",
    "                                                round(min_TPR, 4),\n",
    "                                                round(di, 4),\n",
    "                                                round(eo, 4),\n",
    "                                                round((1-eo/max_TPR), 4),\n",
    "                                                round((1-di)*max_TPR, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3561d3e-c8e8-478b-8858-d21d2b340ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_d10[model_clas] = {'acc':acc,'eo':eo,'di':di,'TPRs':TPRs}\n",
    "results_nofairness[diameter][model_clas.alias] = dict()\n",
    "results_nofairness[diameter][model_clas.alias]['acc'] = acc\n",
    "results_nofairness[diameter][model_clas.alias]['eo'] = eo\n",
    "results_nofairness[diameter][model_clas.alias]['di']= di"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f789ed3-c391-4639-b04e-0592db95c9e0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Method: Optimize TPR - FPR (db.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942fed5c-f143-47c8-a5d6-c9160af747e9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "b_sigmoid=100\n",
    "sum_threshold=0.8\n",
    "model_clas = ETC_optimizeFTPR_db()\n",
    "model_clas.fit(X_clas_train, \n",
    "               y_clas_train,\n",
    "               start_thresholds=start_thresholds,\n",
    "               b_sigmoid=b_sigmoid, \n",
    "               sum_threshold=sum_threshold,\n",
    "               print_coeff=True)\n",
    "acc,eo,di,TPRs = evaluate(model_clas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1160a259-0a6a-4739-8f63-5fb873e13476",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_d10[model_clas] = {'acc':acc,'eo':eo,'di':di,'TPRs':TPRs}\n",
    "results_nofairness[diameter][model_clas.alias] = dict()\n",
    "results_nofairness[diameter][model_clas.alias]['acc'] = acc\n",
    "results_nofairness[diameter][model_clas.alias]['eo'] = eo\n",
    "results_nofairness[diameter][model_clas.alias]['di']= di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6947bb0b-b263-4061-a95f-85362e27fb68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_, y_pred_approx = model_clas.predict_approx(X_clas_train,\n",
    "                                             b_sigmoid=b_sigmoid, \n",
    "                                             sum_threshold=sum_threshold)\n",
    "\n",
    "sensitive_features = list(X_sen_train.columns)\n",
    "for sensitive_feature in sensitive_features:\n",
    "    x_sen = X_sen_train.loc[:,sensitive_feature]\n",
    "    print('Sensitive feature:', sensitive_feature)\n",
    "    print(Cov(x_sen, y_pred_approx.loc[:,'y']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66617c0-a7d8-4bf2-b03c-a25136ad1e73",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Method: Optimize TPR - FPR while enhancing fairness (db.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc30cd46-03c6-467b-a659-44cf56990843",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "b_sigmoid=100\n",
    "sum_threshold=0.8\n",
    "model_clas = ETC_optimizeFTPR_F_db()\n",
    "model_clas.fit(X_clas_train,\n",
    "               X_sen_train,\n",
    "               y_clas_train,\n",
    "               start_thresholds=start_thresholds,\n",
    "               b_sigmoid=b_sigmoid, \n",
    "               sum_threshold=sum_threshold,\n",
    "               c=0.11, #0.07, 0.08, 0.09, 0.1, 0.11!, 0.12 ..., 0.22\n",
    "               mu=0.2, #0.2\n",
    "               print_coeff=True)\n",
    "acc,eo,di,TPRs = evaluate(model_clas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2bde00-bd5d-4584-8832-9b08966847cd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- specify hyperparameters and model class\n",
    "start_hyper = 0.07\n",
    "extra_hypers = []\n",
    "b_sigmoid = 100\n",
    "sum_threshold = 0.8\n",
    "model_clas = ETC_optimizeFTPR_F_db()\n",
    "\n",
    "# --- define constant for stopping criterium\n",
    "comparison_algo = comparisons[model_clas.alias]\n",
    "di_nofairness = results_nofairness[diameter][comparison_algo]['di']\n",
    "print('disparate impact of approx. {} '\\\n",
    "      'from {} algorithm is used as a stopping criterium.'.format(round(di_nofairness,5),\n",
    "                                                                        comparison_algo))\n",
    "\n",
    "# ----- test model for different hyperparameters\n",
    "Cs = extra_hypers + [round(start_hyper+i*0.01,2) for i in range(0,40)]\n",
    "print('\\nHyperparameters to test:\\n', Cs)\n",
    "results_fairness[diameter][model_clas.alias] = dict()\n",
    "results_fairness[diameter][model_clas.alias]['ACCs'] = list()\n",
    "results_fairness[diameter][model_clas.alias]['EOs'] = list()\n",
    "results_fairness[diameter][model_clas.alias]['DIs'] = list()\n",
    "results_fairness[diameter][model_clas.alias]['Cs'] = list()\n",
    "for c in Cs:\n",
    "    print('\\nc:', c)\n",
    "    # --- train model for fixed hyperparameters\n",
    "    model_clas = ETC_optimizeFTPR_F_db()\n",
    "    model_clas.fit(X_clas_train,\n",
    "                   X_sen_train,\n",
    "                   y_clas_train,\n",
    "                   start_thresholds=start_thresholds,\n",
    "                   b_sigmoid=b_sigmoid, \n",
    "                   sum_threshold=sum_threshold,\n",
    "                   c=c, #0.07, 0.08, 0.09, 0.1, 0.11!, 0.12 ..., 0.22\n",
    "                   mu=0.2, #0.2\n",
    "                   print_coeff=False)\n",
    "    # --- evaluate model for fixed hyperparameters\n",
    "    acc,eo,di,TPRs = evaluate(model_clas)\n",
    "    # --- store evaluation until model is as unfair as comparison model\n",
    "    if di <= di_nofairness:\n",
    "        print('\\nHyperparameter {} and larger were not used'.format(c))\n",
    "        break\n",
    "    results_d10[model_clas] = {'acc':acc,'eo':eo,'di':di,'TPRs':TPRs}\n",
    "    results_fairness[diameter][model_clas.alias]['ACCs'].append(acc)\n",
    "    results_fairness[diameter][model_clas.alias]['EOs'].append(eo)\n",
    "    results_fairness[diameter][model_clas.alias]['DIs'].append(di)\n",
    "    results_fairness[diameter][model_clas.alias]['Cs'].append(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12749b5e-8b49-4815-9a9a-c69bbec59a12",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Method: Optimize TPR - FPR  (ndb.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f878f049-e47d-4380-a86d-9242a49e5792",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model_clas = ETC_optimizeFTPR_ndb()\n",
    "#model_clas.fit(X_clas_train, \n",
    "#               y_clas_train,\n",
    "#               start_thresholds=start_thresholds,\n",
    "#               print_coeff=True)\n",
    "#acc,eo,di,TPRs = evaluate(model_clas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e0f0b1-cea2-4d3e-8d75-b6a65f5bf626",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Method: Optimize TPR - FPR while enhancing fairness (ndb.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37df802-c358-4bfa-b2a2-fd7835f5730e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model_clas = ETC_optimizeFTPR_F_ndb()\n",
    "#model_clas.fit(X_clas_train, \n",
    "#               X_sen_train,\n",
    "#               y_clas_train,\n",
    "#               start_thresholds=start_thresholds,\n",
    "#               c=0.11, #0.07, 0.08, 0.09, 0.1, 0.11!, 0.12 ..., 0.33\n",
    "#               mu=0.25, #0.25\n",
    "#               print_coeff=True)\n",
    "#acc,eo,di,TPRs = evaluate(model_clas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198b2764-c3e4-479d-bb98-fb20f1a76425",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Method: Optimize ACC (db.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f539aeb-b2d9-4ee0-9bed-74b8c3bff387",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "b_sigmoid=100\n",
    "sum_threshold=0.8\n",
    "model_clas = ETC_optimizeACC_db()\n",
    "model_clas.fit(X_clas_train,\n",
    "               y_clas_train,\n",
    "               start_thresholds=start_thresholds,\n",
    "               b_sigmoid=b_sigmoid, \n",
    "               sum_threshold=sum_threshold,\n",
    "               print_coeff=True)\n",
    "acc,eo,di,TPRs = evaluate(model_clas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c61369-0ba1-4b78-9157-d8d09b2dbcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_d10[model_clas] = {'acc':acc,'eo':eo,'di':di,'TPRs':TPRs}\n",
    "results_nofairness[diameter][model_clas.alias] = dict()\n",
    "results_nofairness[diameter][model_clas.alias]['acc'] = acc\n",
    "results_nofairness[diameter][model_clas.alias]['eo'] = eo\n",
    "results_nofairness[diameter][model_clas.alias]['di']= di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb7a1c2-fa5c-4fd8-9ff7-679f1b5e0d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_,_,_,_,acc_best,_ = model_clas.score(X_clas_train, \n",
    "                                        y_clas_train,\n",
    "                                        print_all_scores=False)\n",
    "acc_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1b49d8-d8a3-48eb-a54f-bc88cdc76970",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_, y_pred_approx = model_clas.predict_approx(X_clas_train,\n",
    "                                             b_sigmoid=b_sigmoid, \n",
    "                                             sum_threshold=sum_threshold)\n",
    "\n",
    "sensitive_features = list(X_sen_train.columns)\n",
    "for sensitive_feature in sensitive_features:\n",
    "    x_sen = X_sen_train.loc[:,sensitive_feature]\n",
    "    print('Sensitive feature:', sensitive_feature)\n",
    "    print(Cov(x_sen, y_pred_approx.loc[:,'y']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a673ec-d27a-4f28-92b8-50885a550705",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Method: Optimize ACC while enhancing fairness (db.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebdbedf-9f7b-4c01-92a3-4f989c3d57ba",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "b_sigmoid=100\n",
    "sum_threshold=0.8\n",
    "model_clas = ETC_optimizeACC_F_db()\n",
    "model_clas.fit(X_clas_train,\n",
    "               X_sen_train,\n",
    "               y_clas_train,\n",
    "               start_thresholds=start_thresholds,\n",
    "               b_sigmoid=b_sigmoid, \n",
    "               sum_threshold=sum_threshold,\n",
    "               c=0.09, #0.065, 0.07, 0.08, 0.09!, 0.1, ..., 0.14\n",
    "               mu=0.05, #0.05!, 0.1\n",
    "               print_coeff=True)\n",
    "acc,eo,di,TPRs = evaluate(model_clas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854d4c76-c26e-4e5b-801b-c99583f752fc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- specify hyperparameters and model class\n",
    "start_hyper = 0.07\n",
    "extra_hypers = [0.065]\n",
    "b_sigmoid=100\n",
    "sum_threshold=0.8\n",
    "model_clas = ETC_optimizeACC_F_db()\n",
    "\n",
    "# --- define constant for stopping criterium\n",
    "comparison_algo = comparisons[model_clas.alias]\n",
    "di_nofairness = results_nofairness[diameter][comparison_algo]['di']\n",
    "print('disparate impact of approx. {} '\\\n",
    "      'from {} algorithm is used as a stopping criterium.'.format(round(di_nofairness,5),\n",
    "                                                                        comparison_algo))\n",
    "# ----- test model for different hyperparameters\n",
    "Cs = extra_hypers + [round(start_hyper+i*0.01,2) for i in range(0,40)]\n",
    "print('\\nHyperparameters to test:\\n', Cs)\n",
    "results_fairness[diameter][model_clas.alias] = dict()\n",
    "results_fairness[diameter][model_clas.alias]['ACCs'] = list()\n",
    "results_fairness[diameter][model_clas.alias]['EOs'] = list()\n",
    "results_fairness[diameter][model_clas.alias]['DIs'] = list()\n",
    "results_fairness[diameter][model_clas.alias]['Cs'] = list()\n",
    "for c in Cs:\n",
    "    print('\\nc:', c)\n",
    "    # --- train model for fixed hyperparameters\n",
    "    model_clas = ETC_optimizeACC_F_db()\n",
    "    model_clas.fit(X_clas_train,\n",
    "                   X_sen_train,\n",
    "                   y_clas_train,\n",
    "                   start_thresholds=start_thresholds,\n",
    "                   b_sigmoid=b_sigmoid, \n",
    "                   sum_threshold=sum_threshold,\n",
    "                   c=c, #0.065, 0.07, 0.08, 0.09!, 0.1, ..., 0.14\n",
    "                   mu=0.05, #0.05!, 0.1\n",
    "                   print_coeff=False)\n",
    "    # --- evaluate model for fixed hyperparameters\n",
    "    acc,eo,di,TPRs = evaluate(model_clas)\n",
    "    # --- store evaluation until model is as unfair as comparison model\n",
    "    if di <= di_nofairness:\n",
    "        print('\\nHyperparameter {} and larger were not used'.format(c))\n",
    "        break\n",
    "    results_d10[model_clas] = {'acc':acc,'eo':eo,'di':di,'TPRs':TPRs}\n",
    "    results_fairness[diameter][model_clas.alias]['ACCs'].append(acc)\n",
    "    results_fairness[diameter][model_clas.alias]['EOs'].append(eo)\n",
    "    results_fairness[diameter][model_clas.alias]['DIs'].append(di)\n",
    "    results_fairness[diameter][model_clas.alias]['Cs'].append(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca48fddb-9b92-4de6-a5b3-96d460048031",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Method: Optimize ACC (ndb.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb8b8c8-0da3-48bd-8caa-0c8fe77e394a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model_clas = ETC_optimizeACC_ndb()\n",
    "#model_clas.fit(X_clas_train, \n",
    "#               y_clas_train,\n",
    "#               start_thresholds=start_thresholds,\n",
    "#               print_coeff=True)\n",
    "#acc,eo,di,TPRs = evaluate(model_clas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8f8886-8a9e-4849-a5b6-325ef52cadb6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Method: Optimize ACC while enhancing fairness (ndb.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4e828e-8c21-4ad5-9605-14a0ed6afa4c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model_clas = ETC_optimizeACC_F_ndb()\n",
    "#model_clas.fit(X_clas_train, \n",
    "#               X_sen_train,\n",
    "#               y_clas_train,\n",
    "#               start_thresholds=start_thresholds,\n",
    "#               c=0.09, #0.07, 0.08, 0.09!, 0.1, ..., 0.14\n",
    "#               mu=0.3, #0.3\n",
    "#               print_coeff=True)\n",
    "#acc,eo,di,TPRs = evaluate(model_clas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e227f30c-6ab0-4e8b-90b3-62191b6115df",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Method: Optimize DI (ndb.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05389c7e-83f0-4ddf-a2db-d563bed55548",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d04769-56bd-4924-9f9c-1c2c948b172f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_clas = ETC_optimizeDI_ndb()\n",
    "model_clas.fit(X_clas_train,\n",
    "               X_sen_train,\n",
    "               y_clas_train,\n",
    "               start_thresholds=start_thresholds,\n",
    "               mu=0.025, #0.025\n",
    "               lamb=0.03, #0.03!, 0.04, ..., 0.45\n",
    "               acc_best=acc_best,\n",
    "               print_coeff=True)\n",
    "acc,eo,di,TPRs = evaluate(model_clas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edc63e5-dbbd-4a33-b1d0-7151d893a4fe",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- specify hyperparameters and model class\n",
    "start_hyper = 0.45\n",
    "end_hyper = 0.03\n",
    "model_clas = ETC_optimizeDI_ndb()\n",
    "\n",
    "# --- define constant for stopping criterium\n",
    "comparison_algo = comparisons[model_clas.alias]\n",
    "di_nofairness = results_nofairness[diameter][comparison_algo]['di']\n",
    "print('disparate impact of approx. {} '\\\n",
    "      'from {} algorithm is used as a stopping criterium.'.format(round(di_nofairness,5),\n",
    "                                                                        comparison_algo))\n",
    "\n",
    "# --- test model for different hyperparameters\n",
    "Cs = [round(start_hyper-i*0.01,2) for i in range(0,50) if round(start_hyper-i*0.01,2)>=end_hyper]\n",
    "print('\\nHyperparameters to test:\\n', Cs)\n",
    "results_fairness[diameter][model_clas.alias] = dict()\n",
    "results_fairness[diameter][model_clas.alias]['ACCs'] = list()\n",
    "results_fairness[diameter][model_clas.alias]['EOs'] = list()\n",
    "results_fairness[diameter][model_clas.alias]['DIs'] = list()\n",
    "results_fairness[diameter][model_clas.alias]['Lambdas'] = list()\n",
    "for lamb in Cs:\n",
    "    print('\\nlambda:', lamb)\n",
    "    # --- train model for fixed hyperparameters\n",
    "    model_clas = ETC_optimizeDI_ndb()\n",
    "    model_clas.fit(X_clas_train,\n",
    "                   X_sen_train,\n",
    "                   y_clas_train,\n",
    "                   start_thresholds=start_thresholds,\n",
    "                   mu=0.025, #0.025\n",
    "                   lamb=lamb, #0.03!, 0.04, ..., 0.45\n",
    "                   acc_best=acc_best,\n",
    "                   print_coeff=False)\n",
    "    # --- evaluate model for fixed hyperparameters\n",
    "    acc,eo,di,TPRs = evaluate(model_clas)\n",
    "    # --- store evaluation until model is as unfair as comparison model\n",
    "    if di <= di_nofairness:\n",
    "        print('\\nHyperparameter {} and larger were not used'.format(c))\n",
    "        break\n",
    "    results_d10[model_clas] = {'acc':acc,'eo':eo,'di':di,'TPRs':TPRs}\n",
    "    results_fairness[diameter][model_clas.alias]['ACCs'].append(acc)\n",
    "    results_fairness[diameter][model_clas.alias]['EOs'].append(eo)\n",
    "    results_fairness[diameter][model_clas.alias]['DIs'].append(di)\n",
    "    results_fairness[diameter][model_clas.alias]['Lambdas'].append(lamb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8612edd-19cc-4422-bfa2-dba2f9e332c1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Method: Optimize EO (ndb.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c78cc1-1f21-4344-90a3-fea35c3841a1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model_clas = ETC_optimizeEO_ndb()\n",
    "#model_clas.fit(X_clas_train,\n",
    "#               X_sen_train,\n",
    "#               y_clas_train,\n",
    "#               start_thresholds=start_thresholds,\n",
    "#               mu=0.025, #0.025\n",
    "#               lamb=0.03, #0.03!, 0.04, ..., 0.45\n",
    "#               acc_best=acc_best,\n",
    "#               print_coeff=True)\n",
    "#acc,eo,di,TPRs = evaluate(model_clas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbe83b3-78ee-46c2-9a02-8fcaa157e767",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Diameter = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3d5f2b-0404-4c44-8bd0-be8ad9b54193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the training and test data according to the diameter\n",
    "diameter = 15\n",
    "results = filter_diameter(X_clas, y_clas, X_sen, diameter=diameter, df_information=df_information)\n",
    "X_clas_train, X_clas_test, y_clas_train, y_clas_test, X_sen_train, X_sen_test = results\n",
    "print(X_sen_train.sum())\n",
    "print(X_sen_test.sum())\n",
    "\n",
    "# create dictionary to store all results \n",
    "# which are visualized at the end\n",
    "results_d15 = dict()\n",
    "results_fairness[diameter] = dict()\n",
    "results_nofairness[diameter] = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd460bd-1816-4c00-98c6-54d39d3be69e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Method: Choose hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638f87a7-d131-41e8-9d82-38efad0747fd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_clas = ETC_hyperparameter()\n",
    "model_clas.fit(X_clas_train, factor=0.2, print_coeff=True)\n",
    "acc,eo,di,TPRs = evaluate(model_clas)\n",
    "# define the starting point for all other algorithms\n",
    "start_thresholds = model_clas.thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c82fbd-cf7a-4892-81a0-9bd9a4bbc304",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_TPR = max(list(TPRs.values()))\n",
    "min_TPR = min(list(TPRs.values()))\n",
    "print('{} & {} & {} & {} & {} & {} & {}'.format(round(acc, 4),\n",
    "                                                round(max_TPR, 4),\n",
    "                                                round(min_TPR, 4),\n",
    "                                                round(di, 4),\n",
    "                                                round(eo, 4),\n",
    "                                                round((1-eo/max_TPR), 4),\n",
    "                                                round((1-di)*max_TPR, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827ce34a-29ed-46c1-bdbb-ca127185895f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_d15[model_clas] = {'acc':acc,'eo':eo,'di':di,'TPRs':TPRs}\n",
    "results_nofairness[diameter][model_clas.alias] = dict()\n",
    "results_nofairness[diameter][model_clas.alias]['acc'] = acc\n",
    "results_nofairness[diameter][model_clas.alias]['eo'] = eo\n",
    "results_nofairness[diameter][model_clas.alias]['di']= di"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35327dae-6457-41f1-aa45-b65ae58002c3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Method: Optimize TPR - FPR (db.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30910327-f3ee-409b-bf73-155a711c6acd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "b_sigmoid=100\n",
    "sum_threshold=0.8\n",
    "model_clas = ETC_optimizeFTPR_db()\n",
    "model_clas.fit(X_clas_train, \n",
    "               y_clas_train,\n",
    "               start_thresholds=start_thresholds,\n",
    "               b_sigmoid=b_sigmoid, \n",
    "               sum_threshold=sum_threshold,\n",
    "               print_coeff=True)\n",
    "acc,eo,di,TPRs = evaluate(model_clas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fc1bf9-e447-4c1a-a212-1d6dad859c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_d15[model_clas] = {'acc':acc,'eo':eo,'di':di,'TPRs':TPRs}\n",
    "results_nofairness[diameter][model_clas.alias] = dict()\n",
    "results_nofairness[diameter][model_clas.alias]['acc'] = acc\n",
    "results_nofairness[diameter][model_clas.alias]['eo'] = eo\n",
    "results_nofairness[diameter][model_clas.alias]['di']= di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c3190b-81f0-4c36-8732-0a959b613eaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_, y_pred_approx = model_clas.predict_approx(X_clas_train,\n",
    "                                             b_sigmoid=b_sigmoid, \n",
    "                                             sum_threshold=sum_threshold)\n",
    "\n",
    "sensitive_features = list(X_sen_train.columns)\n",
    "for sensitive_feature in sensitive_features:\n",
    "    x_sen = X_sen_train.loc[:,sensitive_feature]\n",
    "    print('Sensitive feature:', sensitive_feature)\n",
    "    print(Cov(x_sen, y_pred_approx.loc[:,'y']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ef9e21-d353-4464-85d8-8636f2455e53",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Method: Optimize TPR - FPR while enhancing fairness (db.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d2050d-02fa-4155-a4a8-82e061e54898",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "b_sigmoid=100\n",
    "sum_threshold=0.8\n",
    "model_clas = ETC_optimizeFTPR_F_db()\n",
    "model_clas.fit(X_clas_train,\n",
    "               X_sen_train,\n",
    "               y_clas_train,\n",
    "               start_thresholds=start_thresholds,\n",
    "               b_sigmoid=b_sigmoid, \n",
    "               sum_threshold=sum_threshold,\n",
    "               c=0.11, #0.09, 0.1, 0.11!, 0.12, ..., 0.17\n",
    "               mu=0.2, #0.2\n",
    "               print_coeff=True)\n",
    "acc,eo,di,TPRs = evaluate(model_clas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736c6c8a-5c27-42c1-8ee1-ede1a1bf7ee6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- specify hyperparameters and model class\n",
    "start_hyper = 0.09\n",
    "extra_hypers = []\n",
    "b_sigmoid = 100\n",
    "sum_threshold = 0.8\n",
    "model_clas = ETC_optimizeFTPR_F_db()\n",
    "\n",
    "# --- define constant for stopping criterium\n",
    "comparison_algo = comparisons[model_clas.alias]\n",
    "di_nofairness = results_nofairness[diameter][comparison_algo]['di']\n",
    "print('disparate impact of approx. {} '\\\n",
    "      'from {} algorithm is used as a stopping criterium.'.format(round(di_nofairness,5),\n",
    "                                                                        comparison_algo))\n",
    "# ----- test model for different hyperparameters\n",
    "Cs = extra_hypers + [round(start_hyper+i*0.01,2) for i in range(0,40)]\n",
    "print('\\nHyperparameters to test:\\n', Cs)\n",
    "results_fairness[diameter][model_clas.alias] = dict()\n",
    "results_fairness[diameter][model_clas.alias]['ACCs'] = list()\n",
    "results_fairness[diameter][model_clas.alias]['EOs'] = list()\n",
    "results_fairness[diameter][model_clas.alias]['DIs'] = list()\n",
    "results_fairness[diameter][model_clas.alias]['Cs'] = list()\n",
    "for c in Cs:\n",
    "    print('\\nc:', c)\n",
    "    # --- train model for fixed hyperparameters\n",
    "    model_clas = ETC_optimizeFTPR_F_db()\n",
    "    model_clas.fit(X_clas_train,\n",
    "                   X_sen_train,\n",
    "                   y_clas_train,\n",
    "                   start_thresholds=start_thresholds,\n",
    "                   b_sigmoid=b_sigmoid, \n",
    "                   sum_threshold=sum_threshold,\n",
    "                   c=c, #0.09, 0.1, 0.11!, 0.12, ..., 0.17\n",
    "                   mu=0.2, #0.2\n",
    "                   print_coeff=False)\n",
    "    # --- evaluate model for fixed hyperparameters\n",
    "    acc,eo,di,TPRs = evaluate(model_clas)\n",
    "    # --- store evaluation until model is as unfair as comparison model\n",
    "    if di <= di_nofairness:\n",
    "        print('\\nHyperparameter {} and larger were not used'.format(c))\n",
    "        break\n",
    "    results_d15[model_clas] = {'acc':acc,'eo':eo,'di':di,'TPRs':TPRs}\n",
    "    results_fairness[diameter][model_clas.alias]['ACCs'].append(acc)\n",
    "    results_fairness[diameter][model_clas.alias]['EOs'].append(eo)\n",
    "    results_fairness[diameter][model_clas.alias]['DIs'].append(di)\n",
    "    results_fairness[diameter][model_clas.alias]['Cs'].append(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d191b25d-afb8-4417-95ff-061d6c016011",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Method: Optimize TPR - FPR  (ndb.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351443da-d644-4f7f-a672-cd7ede4e8faf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model_clas = ETC_optimizeFTPR_ndb()\n",
    "#model_clas.fit(X_clas_train, \n",
    "#               y_clas_train,\n",
    "#               start_thresholds=start_thresholds,\n",
    "#               print_coeff=True)\n",
    "#acc,eo,di,TPRs = evaluate(model_clas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cfda67-ba49-425e-b943-71da274c23fd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Method: Optimize TPR - FPR while enhancing fairness (ndb.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb1c555-dc29-4381-bcac-55d2be7dc44b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model_clas = ETC_optimizeFTPR_F_ndb()\n",
    "#model_clas.fit(X_clas_train, \n",
    "#               X_sen_train,\n",
    "#               y_clas_train,\n",
    "#               start_thresholds=start_thresholds,\n",
    "#               cc=0.11, #0.09, 0.1, 0.11!, 0.12, ..., 0.2\n",
    "#               mu=0.25, #0.25\n",
    "#               print_coeff=True)\n",
    "#acc,eo,di,TPRs = evaluate(model_clas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9676d19-9417-455d-8908-dcf94ce23097",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Method: Optimize ACC (db.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0aad01-3abc-43d5-8afb-474571eee0aa",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "b_sigmoid=100\n",
    "sum_threshold=0.8\n",
    "model_clas = ETC_optimizeACC_db()\n",
    "model_clas.fit(X_clas_train,\n",
    "               y_clas_train,\n",
    "               start_thresholds=start_thresholds,\n",
    "               b_sigmoid=b_sigmoid, \n",
    "               sum_threshold=sum_threshold,\n",
    "               print_coeff=True)\n",
    "acc,eo,di,TPRs = evaluate(model_clas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09cfb4b-e6df-48e2-8706-766285909197",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_d15[model_clas] = {'acc':acc,'eo':eo,'di':di,'TPRs':TPRs}\n",
    "results_nofairness[diameter][model_clas.alias] = dict()\n",
    "results_nofairness[diameter][model_clas.alias]['acc'] = acc\n",
    "results_nofairness[diameter][model_clas.alias]['eo'] = eo\n",
    "results_nofairness[diameter][model_clas.alias]['di']= di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fdd0b1-a4df-4e86-98fe-d46ebe9f089b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_,_,_,_,_,acc_best,_ = model_clas.score(X_clas_train, \n",
    "                                        y_clas_train,\n",
    "                                        print_all_scores=False)\n",
    "acc_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3541b812-2847-4958-a180-65d6af72a75e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_, y_pred_approx = model_clas.predict_approx(X_clas_train,\n",
    "                                             b_sigmoid=b_sigmoid, \n",
    "                                             sum_threshold=sum_threshold)\n",
    "\n",
    "sensitive_features = list(X_sen_train.columns)\n",
    "for sensitive_feature in sensitive_features:\n",
    "    x_sen = X_sen_train.loc[:,sensitive_feature]\n",
    "    print('Sensitive feature:', sensitive_feature)\n",
    "    print(Cov(x_sen, y_pred_approx.loc[:,'y']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db1f6b0-6d7b-442c-808e-9fd1f879135f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Method: Optimize ACC while enhancing fairness (db.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dedbf44-5cfa-4a0d-afcc-8b4216151460",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "b_sigmoid=100\n",
    "sum_threshold=0.8\n",
    "model_clas = ETC_optimizeACC_F_db()\n",
    "model_clas.fit(X_clas_train,\n",
    "               X_sen_train,\n",
    "               y_clas_train,\n",
    "               start_thresholds=start_thresholds,\n",
    "               b_sigmoid=b_sigmoid, \n",
    "               sum_threshold=sum_threshold,\n",
    "               c=0.09, #0.082, 0.085, 0.09!, 0.1, 0.11, 0.12\n",
    "               mu=0.05, #0.05\n",
    "               print_coeff=True)\n",
    "acc,eo,di,TPRs = evaluate(model_clas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a45e18-43da-44de-8eaa-229168bf02af",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- specify hyperparameters and model class\n",
    "start_hyper = 0.09\n",
    "extra_hypers = [0.082, 0.085]\n",
    "b_sigmoid=100\n",
    "sum_threshold=0.8\n",
    "model_clas = ETC_optimizeACC_F_db()\n",
    "\n",
    "# --- define constant for stopping criterium\n",
    "comparison_algo = comparisons[model_clas.alias]\n",
    "di_nofairness = results_nofairness[diameter][comparison_algo]['di']\n",
    "print('disparate impact of approx. {} '\\\n",
    "      'from {} algorithm is used as a stopping criterium.'.format(round(di_nofairness,5),\n",
    "                                                                        comparison_algo))\n",
    "# ----- test model for different hyperparameters\n",
    "Cs = extra_hypers + [round(start_hyper+i*0.01,2) for i in range(0,40)]\n",
    "print('\\nHyperparameters to test:\\n', Cs)\n",
    "results_fairness[diameter][model_clas.alias] = dict()\n",
    "results_fairness[diameter][model_clas.alias]['ACCs'] = list()\n",
    "results_fairness[diameter][model_clas.alias]['EOs'] = list()\n",
    "results_fairness[diameter][model_clas.alias]['DIs'] = list()\n",
    "results_fairness[diameter][model_clas.alias]['Cs'] = list()\n",
    "for c in Cs:\n",
    "    print('\\nc:', c)\n",
    "    # --- train model for fixed hyperparameters\n",
    "    model_clas = ETC_optimizeACC_F_db()\n",
    "    model_clas.fit(X_clas_train,\n",
    "                   X_sen_train,\n",
    "                   y_clas_train,\n",
    "                   start_thresholds=start_thresholds,\n",
    "                   b_sigmoid=b_sigmoid, \n",
    "                   sum_threshold=sum_threshold,\n",
    "                   c=c, #0.082, 0.085, 0.09!, 0.1, 0.11, 0.12\n",
    "                   mu=0.05, #0.05\n",
    "                   print_coeff=False)\n",
    "    # --- evaluate model for fixed hyperparameters\n",
    "    acc,eo,di,TPRs = evaluate(model_clas)\n",
    "    # --- store evaluation until model is as unfair as comparison model\n",
    "    if di <= di_nofairness:\n",
    "        print('\\nHyperparameter {} and larger were not used'.format(c))\n",
    "        break\n",
    "    results_d15[model_clas] = {'acc':acc,'eo':eo,'di':di,'TPRs':TPRs}\n",
    "    results_fairness[diameter][model_clas.alias]['ACCs'].append(acc)\n",
    "    results_fairness[diameter][model_clas.alias]['EOs'].append(eo)\n",
    "    results_fairness[diameter][model_clas.alias]['DIs'].append(di)\n",
    "    results_fairness[diameter][model_clas.alias]['Cs'].append(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e04785-8ab4-4adb-a47f-4100b59a48e6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Method: Optimize ACC (ndb.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7bcc63-5bcf-440b-8c2b-801487b10599",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model_clas = ETC_optimizeACC_ndb()\n",
    "#model_clas.fit(X_clas_train, \n",
    "#               y_clas_train,\n",
    "#               start_thresholds=start_thresholds,\n",
    "#               print_coeff=True)\n",
    "#acc,eo,di,TPRs = evaluate(model_clas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85bd8ca-dbfa-4cca-ad0d-dcdb221b18cf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Method: Optimize ACC while enhancing fairness (ndb.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458c7230-8eee-4fe9-95af-a6ce21b9be01",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model_clas = ETC_optimizeACC_F_ndb()\n",
    "#model_clas.fit(X_clas_train, \n",
    "#               X_sen_train,\n",
    "#               y_clas_train,\n",
    "#               start_thresholds=start_thresholds,\n",
    "#               c=0.09, #0.082, 0,085, 0.09!, 0.1, ..., 0.17\n",
    "#               mu=0.05, #0.05\n",
    "#               print_coeff=True)\n",
    "#acc,eo,di,TPRs = evaluate(model_clas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4998165-49c3-4ec3-a513-c28b7573cae9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Method: Optimize DI (ndb.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa0e81a-e1f8-4cdb-aa6d-14871a1ec50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289733c9-90a4-465c-aa3b-53a2a31016e3",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_clas = ETC_optimizeDI_ndb()\n",
    "model_clas.fit(X_clas_train,\n",
    "               X_sen_train,\n",
    "               y_clas_train,\n",
    "               start_thresholds=start_thresholds,\n",
    "               mu=0.025, #0.025\n",
    "               lamb=0.04, #0.02, 0.03, 0.04!, 0.05, ..., 0.44\n",
    "               acc_best=acc_best,\n",
    "               print_coeff=True)\n",
    "acc,eo,di,TPRs = evaluate(model_clas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e14aee9-2040-48a1-8a98-e8e270d3d9f2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- specify hyperparameters and model class\n",
    "start_hyper = 0.44\n",
    "end_hyper = 0.02\n",
    "model_clas = ETC_optimizeDI_ndb()\n",
    "\n",
    "# --- define constant for stopping criterium\n",
    "comparison_algo = comparisons[model_clas.alias]\n",
    "di_nofairness = results_nofairness[diameter][comparison_algo]['di']\n",
    "print('disparate impact of approx. {} '\\\n",
    "      'from {} algorithm is used as a stopping criterium.'.format(round(di_nofairness,5),\n",
    "                                                                        comparison_algo))\n",
    "\n",
    "# --- test model for different hyperparameters\n",
    "Cs = [round(start_hyper-i*0.01,2) for i in range(0,50) if round(start_hyper-i*0.01,2)>= end_hyper]\n",
    "print('\\nHyperparameters to test:\\n', Cs)\n",
    "results_fairness[diameter][model_clas.alias] = dict()\n",
    "results_fairness[diameter][model_clas.alias]['ACCs'] = list()\n",
    "results_fairness[diameter][model_clas.alias]['EOs'] = list()\n",
    "results_fairness[diameter][model_clas.alias]['DIs'] = list()\n",
    "results_fairness[diameter][model_clas.alias]['Lambdas'] = list()\n",
    "for lamb in Cs:\n",
    "    print('\\nlambda:', lamb)\n",
    "    # --- train model for fixed hyperparameters\n",
    "    model_clas = ETC_optimizeDI_ndb()\n",
    "    model_clas.fit(X_clas_train,\n",
    "                   X_sen_train,\n",
    "                   y_clas_train,\n",
    "                   start_thresholds=start_thresholds,\n",
    "                   mu=0.025, #0.025\n",
    "                   lamb=lamb, #0.02, 0.04!, 0.05, ..., 0.44\n",
    "                   acc_best=acc_best,\n",
    "                   print_coeff=False)\n",
    "    # --- evaluate model for fixed hyperparameters\n",
    "    acc,eo,di,TPRs = evaluate(model_clas)\n",
    "    # --- store evaluation until model is as unfair as comparison model\n",
    "    if di <= di_nofairness:\n",
    "        print('\\nHyperparameter {} and larger were not used'.format(c))\n",
    "        break\n",
    "    results_d15[model_clas] = {'acc':acc,'eo':eo,'di':di,'TPRs':TPRs}\n",
    "    results_fairness[diameter][model_clas.alias]['ACCs'].append(acc)\n",
    "    results_fairness[diameter][model_clas.alias]['EOs'].append(eo)\n",
    "    results_fairness[diameter][model_clas.alias]['DIs'].append(di)\n",
    "    results_fairness[diameter][model_clas.alias]['Lambdas'].append(lamb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ddbf41-d342-409f-bf71-a01ae0138bc1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Method: Optimize EO (ndb.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb35e2bc-a542-4597-bf66-09b41725b090",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model_clas = ETC_optimizeEO_ndb()\n",
    "#model_clas.fit(X_clas_train,\n",
    "#               X_sen_train,\n",
    "#               y_clas_train,\n",
    "#               start_thresholds=start_thresholds,\n",
    "#               mu=0.025, #0.025\n",
    "#               lamb=0.06, #0.01, ..., 0.05, 0.06!, 0.07, ..., 0.44\n",
    "#               acc_best=acc_best,\n",
    "#               print_coeff=True)\n",
    "#acc,eo,di,TPRs = evaluate(model_clas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce454f9c-5ec7-4faa-ba58-c5f695af13e6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa3555b-7bad-4756-a399-df009dcbb26f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "aliase = ['H', 'T-F-PR', 'T-F-PR+F','ACC','ACC+F','DI+ACC']\n",
    "results_d5_final = dict()\n",
    "for key in results_d5.keys():\n",
    "    if key.alias in aliase:\n",
    "        results_d5_final[key] = results_d5[key]\n",
    "        \n",
    "df1, df2, df3, fig_d5 = graphics_bars(results_d5_final,\n",
    "                                      save_figs_d=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b48aa37-9129-4d8c-9c4d-d00d3933d417",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "aliase = ['H', 'T-F-PR', 'T-F-PR+F','ACC','ACC+F','DI+ACC']\n",
    "results_d10_final = dict()\n",
    "for key in results_d10.keys():\n",
    "    if key.alias in aliase:\n",
    "        results_d10_final[key] = results_d10[key]\n",
    "        \n",
    "df1, df2, df3, fig_d10 = graphics_bars(results_d10_final,\n",
    "                                       save_figs_d=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32489682-cfc2-4dbf-9ace-1d9759929c21",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "aliase = ['H', 'T-F-PR', 'T-F-PR+F','ACC','ACC+F','DI+ACC']\n",
    "results_d15_final = dict()\n",
    "for key in results_d15.keys():\n",
    "    if key.alias in aliase:\n",
    "        results_d15_final[key] = results_d15[key]\n",
    "\n",
    "df1, df2, df3, fig_d15 = graphics_bars(results_d15_final,\n",
    "                                       save_figs_d=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b17727-4762-476d-bdde-d9c8d42ada2f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "graphics_scatter(results_fairness, \n",
    "                 results_nofairness,\n",
    "                 comparisons,\n",
    "                 horizontal=False,\n",
    "                 save_figs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a8db98-2545-4f05-8f9c-87d81c640889",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "graphics_lines(results_fairness,\n",
    "               results_nofairness,\n",
    "               comparisons,\n",
    "               with_eo=False,\n",
    "               save_figs=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
